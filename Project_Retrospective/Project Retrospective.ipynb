{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap-Up\n",
    "\n",
    "## Project Retrospective\n",
    "\n",
    "Provide answers to the following questions and submit a PDF via Canvas. Be sure to answer the questions completely and explain your logic.\n",
    "\n",
    "## Q1: \n",
    "**You’ve now had the chance to work with both MapReduce and Spark. In your opinion, what are the pros and cons of both?**\n",
    "\n",
    "**MapReduce Pros:**\n",
    "\n",
    "- Easy to implement.\n",
    "- Relax the developer job.\n",
    "- can (potentially) query live data.\n",
    "- can (conceptually) be highly efficient at joining data sets that are identically sharded on the join key (the joins can be pushed down into the key-value store itself).\n",
    "\n",
    "\n",
    "**MapReduce Cons:**\n",
    "\n",
    "- The design are limited to the 2 phases of map and reduce that limited the developer jobs. \n",
    "- We need to wait to see the result to ensure the process are correct. \n",
    "- Full scans (the most common pattern for map-reduce) is most likely to be much faster with raw file system access.\n",
    "- because of the better decoupling of computation and storage in the Map-Reduce model resulting from MR jobs is much easier.\n",
    "- Unstructured data and need to write writable for the data structure.\n",
    "- key-value stores are rarely arranged to have schemas optimized for analytics.\n",
    "\n",
    "\n",
    "**Spark Pros:**\n",
    "\n",
    "- High speed\n",
    "- Ease of Use\n",
    "- Advanced analytics\n",
    "- Dynamic in Nature\n",
    "- Multilingual\n",
    "- Apache Spark is powerful\n",
    "- Increased access to Big data\n",
    "- Demand for Spark Developers\n",
    "\n",
    "**Spark Cons:**\n",
    "\n",
    "- No automatic optimization processes\n",
    "- File management system\n",
    "- Fewer Algorithm\n",
    "- Small Files Issue\n",
    "- Window Criteria\n",
    "- Doesn’t suit for a multi-user environment\n",
    "- Lazy structure \n",
    "\n",
    "**[Rozita]:** <br/>\n",
    "I personally like to work with Spark, because of easy setup and powerful APIs. However, Spark has poor documentation.  \n",
    "\n",
    "\n",
    "**[Hiep]:**   \n",
    "For me, i think Spark is easier to implement because even though with DataFrame API, it still nearly the same with SQL query. Don't need to care much about what will do in mapper, what will do in reducer, just write query.\n",
    "\n",
    "**[Alper]:** \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2:\n",
    "**Was there something that you thought would be easy to implement in Spark but it turned out that it wasn’t?**\n",
    "\n",
    "Yes!! Not the Spark but the project setup and running time. We did not expect to spends 5 days to run the dataset and keep on getting disconnection and error. The issue was solved using sampled data. \n",
    "In overall working with Spark was fun specially with Jupiter interface that makes it very user friendly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3:\n",
    "**Were there any confusing or surprising aspects of working with Spark? Did you come across some functionality that made your life easier or the computations run faster?**\n",
    "\n",
    "\n",
    "**[Rozita]:**<br/>\n",
    "I personally enjoyed working with machine learning and Pandas library. The implemented API makes the machine learning concept very easy and understandable. Its data representation was extremely streamlined forms of data representation. It helps for less writing code and more work is done.!\n",
    "\n",
    "\n",
    "**[Hiep]:**<br/>\n",
    "While writing in Spark, i found someway to make it faster, at first cache only the data i need because memory in orion is limited :). The way Spark allow to write UDF(User-Define Functions) make it easier to custom aggregate function. Long code of MapReduce is replaced by shorter code in Spark\n",
    "\n",
    "**[Alper]:**<br/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4: \n",
    "**1.\tGive a rough estimate of how long you spent completing this assignment. What part of the assignment took the most time?**\n",
    "\n",
    "Dear Mathew, we learn that your projects always took a long time to be completed. \n",
    "We have started from November 25, 2019 and it is about to be completed on December 8, 2019. Of course, we did not work full time on this projects but it was time consuming. The most time taken was running time to get result specially before sampled data provided. \n",
    "\n",
    "**2.\tWhat went well?**\n",
    "\n",
    "As long as, we learn how to work with spark and learn about reddit data structure, the rest went well. Just need time to complete it and make it as the report. \n",
    "\n",
    "\n",
    "**3.\tWhat didn’t go well?**\n",
    "\n",
    "Use original data and sampling data was the toughest part and waste a lot of time. I believe, our resources are not enough for this volume of data with the current number of users. \n",
    "or in other words, Big Data course resources are not enough for Big Data course.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Questions\n",
    "\n",
    "**If you worked in a team, answer the following:**\n",
    "**1.\tHow did you decide to divide up the workload and coordinate with your team?**\n",
    "\n",
    "Each team member took the responsibility of one question at the time. Warm up questions have been done twice since Rozita joined in midst of the project. \n",
    "We had code review session that after any pull request or raising flag by the team member, the rest of member conduct code review or some discussion on WhatsApp group for better understanding. \n",
    "\n",
    "**2.\tDescribe the questions or deliverables completed by each team member:**\n",
    "\n",
    "**Question** | **Persion In Charge** | **Status**\n",
    "--- | --- | ---\n",
    "W1| Hiep & Rozita  | DONE\n",
    "W2 | Alper & Rozita| DONE\n",
    "W3 | Hiep & Rozita| DONE\n",
    "W4 | Alper & Rozita| DONE\n",
    "A1 | Hiep & Rozita| DONE\n",
    "A2 | Alper | DONE\n",
    "A3 | Hiep | DONE \n",
    "A4 | Alper | DONE\n",
    "A5 | Hiep | DONE\n",
    "A6 | Alper | DONE\n",
    "A7-1 | Rozita | DONE\n",
    "A7-2 | Rozita | DONE\n",
    "A7-3 | Rozita | DONE\n",
    "Retrospective| All Team | DONE\n",
    "\n",
    "\n",
    "We had code review session that after completing by one member, others act as code reviewer in a circle loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
