{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Question-2 Readability \n",
    "\n",
    "Readability: write a job that computes Gunning Fog Index and Flesch-Kincaid Readability (both reading ease and grade level) of user comments. Then:\n",
    "Choose a subreddit and plot the distribution of these scores.\n",
    "Choose two subreddits focused on similar topics but with different views, e.g., /r/apple and /r/android. Use these metrics to compare the populations from both.\n",
    "\n",
    "Lets start our work with giving definitions of Gunning Fog Index and Flesch-Kincaid Readability:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Gunning Fog Index\n",
    "\n",
    "The Gunning fog index is a readability test for English writing. The index estimates the years of formal education a person needs to understand the text on the first reading. For instance, a fog index of 12 requires the reading level of a United States high school senior (around 18 years old). The test was developed in 1952 by Robert Gunning, an American businessman who had been involved in newspaper and textbook publishing.\n",
    "\n",
    "The fog index is commonly used to confirm that text can be read easily by the intended audience. Texts for a wide audience generally need a fog index less than 12. Texts requiring near-universal understanding generally need an index less than 8.\n",
    "\n",
    "\n",
    "*Complex Word: Count the \"complex\" words consisting of three or more syllables. Do not include proper nouns, familiar jargon, or compound words. Do not include common suffixes (such as -es, -ed, or -ing) as a syllable;\n",
    "\n",
    "\n",
    "![alt text](https://i.imgur.com/S1wP7v0.jpg \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://i.imgur.com/3WshESB.jpg \"Logo Title Text 1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Flesch-Kincaid Readability\n",
    "\n",
    "The Flesch–Kincaid readability tests are readability tests designed to indicate how difficult a passage in English is to understand. There are two tests, the Flesch Reading Ease, and the Flesch–Kincaid Grade Level. Although they use the same core measures (word length and sentence length), they have different weighting factors.\n",
    "\n",
    "The results of the two tests correlate approximately inversely: a text with a comparatively high score on the Reading Ease test should have a lower score on the Grade-Level test. Rudolf Flesch devised the Reading Ease evaluation; somewhat later, he and J. Peter Kincaid developed the Grade Level evaluation for the United States Navy.\n",
    "\n",
    "![alt text](https://i.imgur.com/gMdtd3M.jpg \"Logo Title Text 1\")\n",
    "\n",
    "The result is a number that corresponds with a U.S. grade level. The sentence, \"The Australian platypus is seemingly a hybrid of a mammal and reptilian creature\" is an 11.3 as it has 24 syllables and 13 words. The different weighting factors for words per sentence and syllables per word in each scoring system mean that the two schemes are not directly comparable and cannot be converted. The grade level formula emphasises sentence length over word length. By creating one-word strings with hundreds of random characters, grade levels may be attained that are hundreds of times larger than high school completion in the United States. Due to the formula's construction, the score does not have an upper bound.\n",
    "\n",
    "The lowest grade level score in theory is −3.40, but there are few real passages in which every sentence consists of a single one-syllable word. Green Eggs and Ham by Dr. Seuss comes close, averaging 5.7 words per sentence and 1.02 syllables per word, with a grade level of −1.3. (Most of the 50 used words are monosyllabic; \"anywhere\", which occurs eight times, is the only exception.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subreddits we choose\n",
    "\n",
    "We queried these subreddits for our analysis: Soccer and NFL(The National Football League) We decided to get comments from all years for analyzing the data. We used sampled data as you can see in the Implementation part. (So we took the %1 of the total comments in our HDFS.)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://i.imgur.com/B16BVBX.jpg \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen the below distribution chart, Soccer Author's grade level is slightly higher than the NFL Author's grade level. On the other hand, similarity between two sport authors grade level is undeniable. Both Soccer and NFL author grade level is mostly fit between Grade 12 and 15. (High school senior to College sophomore) \n",
    "Highest grade levels (College junior-College graduate) in Soccer has a proportion of 7.2%. On the other hand Highest grade levels in NFL is 5.1%. \n",
    "Lowest grade levels (Sixth grade-Eighth grade) in NFL has a proportion of 28.8%. Lowest grade levels in Soccer is 24.0%. \n",
    "\n",
    "As a result, Soccer authors seems smarter than NFL authors. So that makes us proud as a soccer lovers :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://i.imgur.com/H3LGzvh.jpg \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what about Readability Levels?(Flesch–Kincaid readability tests) \n",
    "\n",
    "In this competition again soccer is slighlty head of its rival NFL, again. Most of the comments in Soccer is \"Fairly Easy\" with proportion of 24.9%.  Most of the comments in NFL is \"Easy\" to read with a proportion of 27.1%. Interesting result in this analysis is in both sports \"Fairly Easy\" proportions are same with 24.9%. \n",
    "\n",
    "Now lets talk about the extreme values. The comments that are \"Very easy\" to read so even an average 11-year-old student can understand easily proportion is 14.8% in NFL. This value is 10.5% in soccer. So what about \"Very Diffult ro read\" comments which are best understood by university graduates? Soccer comments has proportion of 2.7% in this aristocratic competition. On the other hand, NFL has only 1.8% grad level complexity in all comments. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://i.imgur.com/eyXQBYj.jpg \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "originDF = sqlContext.read.json(\"hdfs://orion11:11001/sampled_reddit/*\")\n",
    "df = originDF.sample(False, .1)\n",
    "df.createGlobalTempView(\"Comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def syllable_count(word):\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\"):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def pre_process(text):\n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|[^\\\\w|\\\\s]|(\\_))+\",\"\",text)\n",
    "    text=re.sub(\"(\\\\s)+\",\" \",text)\n",
    "    #print(text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home4/saozdamar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "samp = df.sample(False, .1)\n",
    "#newsamp = samp.filter(df.subreddit.like(\"soccer\")&~samp['author'].isin(['[deleted]'])).limit(5)\n",
    "newsamp = samp.filter(df.subreddit.like(\"soccer\")&~samp['author'].isin(['[deleted]']))\n",
    "#newsamp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"soccer_kincaidIndex.txt\", \"a+\")\n",
    "g=open(\"soccer_fogIndex.txt\", \"a+\")\n",
    "\n",
    "iteratebody = newsamp.select(\"body\").rdd.flatMap(list).collect()\n",
    "for i, row in enumerate(iteratebody):    \n",
    "    complex_words=0\n",
    "    #print(i, \".\", row)\n",
    "    number_of_sentences = sent_tokenize(row)\n",
    "    #print(\"Number Of Sentences=\",len(number_of_sentences))\n",
    "    #print('Total words=   ', len(row.split()))        \n",
    "    stops = row.count('.')\n",
    "    stops = stops+row.count('?')\n",
    "    stops = stops+row.count('!')    \n",
    "    #print('total stops=    ', stops);\n",
    "    row = pre_process(row)\n",
    "    # Cleaning text and lower casing all words\n",
    "    for char in '-.,\\n':\n",
    "        row=row.replace(char,' ')\n",
    "        row = row.lower()\n",
    "        # split returns a list of words delimited by sequences of whitespace (including tabs, newlines, etc, like re's \\s) \n",
    "        word_list = row.split()        \n",
    "    total_syllables=0;    \n",
    "    for i, word in enumerate(word_list):\n",
    "    #    print(i, \".\", word,\",syllableCount=\",syllable_count(word))        \n",
    "        total_syllables = total_syllables+syllable_count(word)\n",
    "        if(syllable_count(word)>=3):\n",
    "            complex_words=complex_words+1            \n",
    "        \n",
    "    total_words=len(row.split())\n",
    "    #print(\"total_syllables:\",total_syllables)\n",
    "    #print(\"complex_words:\",complex_words)    \n",
    "    #print(\"total_sentences:\",len(number_of_sentences))\n",
    "    total_sentences = len(number_of_sentences)\n",
    "    \n",
    "    \n",
    "    if(total_sentences!=0 and total_words!=0):\n",
    "        kincaidIndex = 206.835 - (1.015*(total_words/total_sentences)) - 84.6*(total_syllables/total_words)\n",
    "        #print(\"(total_words/total_sentences):\",(total_words/total_sentences))\n",
    "        #print(\"(total_syllables/total_words):\",(total_syllables/total_words))\n",
    "        #print(\"(complex_words/words):\",(complex_words/total_words))\n",
    "        #print(\"total_words:\" , total_words);\n",
    "        #print(\"KincaidIndex:\",kincaidIndex)    \n",
    "        gunningIndex = 0.4* ((total_words/total_sentences)) + 100*(complex_words/total_words)\n",
    "        #print(\"FogIndex:\",gunningIndex)       \n",
    "        \n",
    "        if(gunningIndex>=6 and gunningIndex<=17):\n",
    "            g.write(\"%d\\r\\n\" % (gunningIndex))    \n",
    "        \n",
    "        if(kincaidIndex>=0 and kincaidIndex<=100):\n",
    "            f.write(\"%d\\r\\n\" % (kincaidIndex))        \n",
    "            \n",
    "f.close();\n",
    "g.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home4/saozdamar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "samp = df.sample(False, .1)\n",
    "#newsamp = samp.filter(df.subreddit.like(\"soccer\")&~samp['author'].isin(['[deleted]'])).limit(5)\n",
    "newsamp = samp.filter(df.subreddit.like(\"nfl\")&~samp['author'].isin(['[deleted]']))\n",
    "#newsamp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"nfl_kincaidIndex.txt\", \"a+\")\n",
    "g=open(\"nfl_fogIndex.txt\", \"a+\")\n",
    "\n",
    "iteratebody = newsamp.select(\"body\").rdd.flatMap(list).collect()\n",
    "for i, row in enumerate(iteratebody):    \n",
    "    complex_words=0\n",
    "    #print(i, \".\", row)\n",
    "    number_of_sentences = sent_tokenize(row)\n",
    "    #print(\"Number Of Sentences=\",len(number_of_sentences))\n",
    "    #print('Total words=   ', len(row.split()))        \n",
    "    stops = row.count('.')\n",
    "    stops = stops+row.count('?')\n",
    "    stops = stops+row.count('!')    \n",
    "    #print('total stops=    ', stops);\n",
    "    row = pre_process(row)\n",
    "    # Cleaning text and lower casing all words\n",
    "    for char in '-.,\\n':\n",
    "        row=row.replace(char,' ')\n",
    "        row = row.lower()\n",
    "        # split returns a list of words delimited by sequences of whitespace (including tabs, newlines, etc, like re's \\s) \n",
    "        word_list = row.split()        \n",
    "    total_syllables=0;    \n",
    "    for i, word in enumerate(word_list):\n",
    "    #    print(i, \".\", word,\",syllableCount=\",syllable_count(word))        \n",
    "        total_syllables = total_syllables+syllable_count(word)\n",
    "        if(syllable_count(word)>=3):\n",
    "            complex_words=complex_words+1            \n",
    "        \n",
    "    total_words=len(row.split())\n",
    "    #print(\"total_syllables:\",total_syllables)\n",
    "    #print(\"complex_words:\",complex_words)    \n",
    "    #print(\"total_sentences:\",len(number_of_sentences))\n",
    "    total_sentences = len(number_of_sentences)\n",
    "    \n",
    "    \n",
    "    if(total_sentences!=0 and total_words!=0):\n",
    "        kincaidIndex = 206.835 - (1.015*(total_words/total_sentences)) - 84.6*(total_syllables/total_words)\n",
    "        #print(\"(total_words/total_sentences):\",(total_words/total_sentences))\n",
    "        #print(\"(total_syllables/total_words):\",(total_syllables/total_words))\n",
    "        #print(\"(complex_words/words):\",(complex_words/total_words))\n",
    "        #print(\"total_words:\" , total_words);\n",
    "        #print(\"KincaidIndex:\",kincaidIndex)    \n",
    "        gunningIndex = 0.4* ((total_words/total_sentences)) + 100*(complex_words/total_words)\n",
    "        #print(\"FogIndex:\",gunningIndex)       \n",
    "        \n",
    "        if(gunningIndex>=6 and gunningIndex<=17):\n",
    "            g.write(\"%d\\r\\n\" % (gunningIndex))    \n",
    "        \n",
    "        if(kincaidIndex>=0 and kincaidIndex<=100):\n",
    "            f.write(\"%d\\r\\n\" % (kincaidIndex))        \n",
    "            \n",
    "f.close();\n",
    "g.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "Output result files can be found in our Github repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Wikipedia : https://en.wikipedia.org/wiki/Gunning_fog_index\n",
    "\n",
    "Wikipedia : https://en.wikipedia.org/wiki/Flesch–Kincaid_readability_tests"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
