{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis-7 Final Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve had the opportunity to analyze two datasets thus far; now it’s time to analyze a dataset of your own. Find a dataset online and use Spark (or Hadoop) to analyze it. You should:\n",
    "\n",
    "1. Describe the dataset (Earthquack)\n",
    "2. Outline the types of insights you hope to gain from it\n",
    "3. Make hypotheses about what you might find\n",
    "4. Design at least 3 “questions” (along the lines of those above) and answer them. Remember that presentation matters here. ML Models are a good choice for some of the datasets; you can describe what you’ll try to predict or classify and outline your experiences with various models.\n",
    "\n",
    "Q1) Howmany people talk in subreddit earthquake and the correlation on a real earthquake\n",
    "\n",
    "Q2) 10 years get data from Earthquake web site. 1990=2310, ,  2006=190, 2007=200, 2008=230  , 2021=Make Prediction.\n",
    "\n",
    "Q3) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction About Upcoming New Reddit User Numbers (Apply ML Model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the given data set, we will find the unique user numbers in reddit for each month and then we will make prediction for upcoming month's unique user numbers. (We will use ML models) By doing this reddit development team can prepare themselves for upcoming load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "spark.catalog.dropGlobalTempView(\"Comments\")\n",
    "df = sqlContext.read.json(\"hdfs://orion11:11001/sampled_reddit_v2/*\")\n",
    "df.createGlobalTempView(\"Comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "botExpr = \"[bB][oO][tT]\"\n",
    "\n",
    "filteredComment = (df\n",
    "                   .filter(~(df.body.like(\"[deleted]\") \n",
    "                             | df.body.like('[removed]') \n",
    "                             | df.author.rlike(botExpr) \n",
    "                             | df.author.like(\"[deleted]\")\n",
    "                            )\n",
    "                          )\n",
    "                  )\n",
    "df.unpersist()\n",
    "filteredComment.cache()\n",
    "print(filteredComment.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add month and year column into the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.sql.functions import year, month, dayofmonth, from_unixtime\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "filteredComment = (filteredComment\n",
    "      .withColumn(\"year\", year(from_unixtime(\"created_utc\").cast(DateType())))\n",
    "      .withColumn(\"month\",month(from_unixtime(\"created_utc\").cast(DateType()))))\n",
    "filteredComment.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of active user for each month and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[month: int, year: int, NumOfUser: bigint]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "userByTime = filteredComment.groupBy('month','year').agg(countDistinct('author').alias('NumOfUser'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---------+\n",
      "|month|year|NumOfUser|\n",
      "+-----+----+---------+\n",
      "|   12|2005|        7|\n",
      "|    1|2006|       29|\n",
      "|    2|2006|       58|\n",
      "|    3|2006|       83|\n",
      "|    4|2006|      129|\n",
      "|    5|2006|      156|\n",
      "|    6|2006|      173|\n",
      "|    7|2006|      212|\n",
      "|    8|2006|      310|\n",
      "|    9|2006|      280|\n",
      "|   10|2006|      325|\n",
      "|   11|2006|      355|\n",
      "|   12|2006|      393|\n",
      "|    1|2007|      461|\n",
      "|    2|2007|      538|\n",
      "|    3|2007|      632|\n",
      "|    4|2007|      671|\n",
      "|    5|2007|      819|\n",
      "|    6|2007|      830|\n",
      "|    7|2007|      984|\n",
      "+-----+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as func\n",
    "userByTime = userByTime.sort(func.asc('year'), func.asc('month'))\n",
    "userByTime.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "userByTime.coalesce(1).write.format('csv').save('hdfs://orion11:41075/A7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.73 ms, sys: 77 µs, total: 3.81 ms\n",
      "Wall time: 3.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "originDF = sqlContext.read.json(\"hdfs://orion11:11001/sampled_reddit_v3/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home4/rteymourzadeh/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPUTING FOR :  alaska\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'originDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-249bb81c6ca8>\u001b[0m in \u001b[0;36mcalculatue_score2\u001b[0;34m(category)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vader_lexicon'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalculatue_score2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mnewsamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubreddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m&\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0moriginDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'author'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'[deleted]'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0miteratebody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewsamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"body\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'originDF' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLED Data\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "#df = sqlContext.read.json(\"hdfs://orion11:41001/sampled_reddit/*\")\n",
    "#df.createOrReplaceTempView(\"comments\")\n",
    "# df.saveAsTable(\"Unique subreddit comments\")\n",
    "\n",
    "# subredditDF = filteredComment.select(\"subreddit\").distinct()\n",
    "\n",
    "subredditDf = filteredComment.groupBy('year','subreddit')\n",
    "subredditDf = (subredditDf\n",
    "               .filter(subredditDf.subreddit.rlike('earthquake'))\n",
    "               .groupBy('year')\n",
    "               .agg(func.count(func.lit(1)).alias('NumofSubReddit')))\n",
    "\n",
    "subredditDf.show()\n",
    "\n",
    "# subredditDF = SubrredditDF.filterBy('subredd')(subreddit\n",
    "#                         .groupBy('year')\n",
    "#                         .agg(func.count('Earthquake').alias('numOfComments')))\n",
    "\n",
    "\n",
    "# sqlDF=spark.sql(\"SELECT count(distinct subreddit) AS Unique_Subreddit from comments\")\n",
    "# sqlDF.show()\n",
    "\n",
    "\n",
    "\n",
    "# df = sqlContext.read.json(\"hdfs://orion11:41001/reddit/*\")\n",
    "# df.createOrReplaceTempView(\"Comments\")\n",
    "\n",
    "# Read from 2005 to 2017\n",
    "\n",
    "# sqlDF=spark.sql(\"SELECT subredit,count(*) AS CommentCount from Comments and subreddit='Earthquake' and (created_utc>=1104552009000 and created_utc<=1328054399)\")\n",
    "\n",
    "# sqlDF.show()\n",
    "# sqlDF.show(df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
