{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis-7 Final Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve had the opportunity to analyze two datasets thus far; now it’s time to analyze a dataset of your own. Find a dataset online and use Spark (or Hadoop) to analyze it. You should:\n",
    "\n",
    "1. Describe the dataset (Earthquack)\n",
    "2. Outline the types of insights you hope to gain from it\n",
    "3. Make hypotheses about what you might find\n",
    "4. Design at least 3 “questions” (along the lines of those above) and answer them. Remember that presentation matters here. ML Models are a good choice for some of the datasets; you can describe what you’ll try to predict or classify and outline your experiences with various models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description \n",
    "\n",
    "**Describe the dataset:**\n",
    "In this Problem Analysis, we have used \"USGS\"science for changing world website as our dataset with the following weblink:\n",
    "[USGS](https://earthquake.usgs.gov/earthquakes/search/)\n",
    "\n",
    "The area under analysis is shown in the following image. ![Area under analysis](earthquake2.png)\n",
    "\n",
    "**Outline the types of insights you hope to gain from it:**\n",
    "After this statistic analysis we hope to gain the following information:\n",
    "\n",
    "1) How much does people pay attention to earthquake issue in the reddit platform\n",
    "2) Predict for estimated number of earthquake may happen in 2020\n",
    "3) Predict for the largest strong earthquake may happen in 2020\n",
    "\n",
    "**Make hypotheses about what you might find:**\n",
    "\n",
    "We expect earthquake topic attracts more attention specially in the year that many earthquakes happen. \n",
    "We expect people talk more about the topic related to earthquake.\n",
    "In addition, we exect the number of happening earthquake and the power of the earthquakes follow a linear behaviour in a single decade. \n",
    "\n",
    "**Design 3 questions:**\n",
    "\n",
    "#### Q1) Howmany people did talk about earthquake in the subreddit during the earthquicl time and the correlation with the real earthquake date?\n",
    "\n",
    "#### Q2) Use Machine Learning to  calculate the maximum number of earthquake in 2020?\n",
    "\n",
    "#### Q3) Use Machine Learning to calculate the strongest earthquick may happen in 2020?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and Filter the Sampled Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the given data set, we will find the unique user numbers in reddit for each month and then we will make prediction for upcoming month's unique user numbers. (We will use ML models) By doing this reddit development team can prepare themselves for upcoming load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 ms, sys: 5.02 ms, total: 10 ms\n",
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark.catalog.dropGlobalTempView(\"Comments\")\n",
    "df = sqlContext.read.json(\"hdfs://orion11:11001/sampled_reddit_v3/*\")\n",
    "df.createGlobalTempView(\"Comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2742841\n",
      "CPU times: user 11.7 ms, sys: 2.55 ms, total: 14.2 ms\n",
      "Wall time: 34.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "botExpr = \"[bB][oO][tT]\"\n",
    "\n",
    "filteredComment = (df\n",
    "                   .filter(~(df.body.like(\"[deleted]\") \n",
    "                             | df.body.like('[removed]') \n",
    "                             | df.author.rlike(botExpr) \n",
    "                             | df.author.like(\"[deleted]\")\n",
    "                            )\n",
    "                          )\n",
    "                  )\n",
    "df.unpersist()\n",
    "filteredComment.cache()\n",
    "print(filteredComment.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add month and year column into the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.4 ms, sys: 3.09 ms, total: 16.5 ms\n",
      "Wall time: 1.19 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2742841"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql.functions import year, month, dayofmonth, from_unixtime\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "filteredComment = (filteredComment\n",
    "      .withColumn(\"year\", year(from_unixtime(\"created_utc\").cast(DateType())))\n",
    "      .withColumn(\"month\",month(from_unixtime(\"created_utc\").cast(DateType()))))\n",
    "filteredComment.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1) Howmany people did talk about earthquake in the subreddit during the earthquicl time and the correlation with the real earthquake date?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have targetted the US regien as shown above and based on the dataset collected from 2006 up to 2017 the number of earthquakes are calculated in the dataset.\n",
    "In addition, we have used the reddit data set to see the number of people who commented about earthquakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.73 ms, sys: 77 µs, total: 3.81 ms\n",
      "Wall time: 3.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "originDF = sqlContext.read.json(\"hdfs://orion11:11001/sampled_reddit_v3/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------+\n",
      "|year|           subreddit|count(1)|\n",
      "+----+--------------------+--------+\n",
      "|2014|          skyrimmods|     136|\n",
      "|2014|                deaf|       8|\n",
      "|2014|               dubai|      30|\n",
      "|2014|                lost|      20|\n",
      "|2014|        raspberry_pi|      59|\n",
      "|2014|terriblefacebookm...|      16|\n",
      "|2014|       ClickerHeroes|      89|\n",
      "|2014|            GameSale|      52|\n",
      "|2014|RandomActsOfChris...|      14|\n",
      "|2014|         RateMyMayor|      90|\n",
      "|2014|              Flyers|      94|\n",
      "|2014|          EmeraldPS2|      19|\n",
      "|2014|     douglovesmovies|       5|\n",
      "|2015|               trees|    2034|\n",
      "|2015|      TumblrInAction|    1815|\n",
      "|2014|          glassheads|      30|\n",
      "|2015|           firewater|       6|\n",
      "|2015|            formula1|     842|\n",
      "|2014|              ukguns|       5|\n",
      "|2015|              Nissan|      10|\n",
      "+----+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+--------------+\n",
      "|year|NumofSubReddit|\n",
      "+----+--------------+\n",
      "+----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SAMPLED Data\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "#df = sqlContext.read.json(\"hdfs://orion11:41001/sampled_reddit/*\")\n",
    "#df.createOrReplaceTempView(\"comments\")\n",
    "# df.saveAsTable(\"Unique subreddit comments\")\n",
    "\n",
    "# subredditDF = filteredComment.select(\"subreddit\").distinct()\n",
    "\n",
    "subredditDf = filteredComment.groupBy('year','subreddit').agg(func.count(func.lit(1)))\n",
    "subredditDf.show()\n",
    "subredditDf = (subredditDf\n",
    "               .filter(subredditDf.subreddit.rlike('earthquake'))\n",
    "               .groupBy('year')\n",
    "               .agg(func.count(func.lit(1)).alias('NumofSubReddit')))\n",
    "\n",
    "subredditDf.show()\n",
    "\n",
    "\n",
    "# subredditDF = SubrredditDF.filterBy('subredd')(subreddit\n",
    "#                         .groupBy('year')\n",
    "#                         .agg(func.count('Earthquake').alias('numOfComments')))\n",
    "\n",
    "\n",
    "# sqlDF=spark.sql(\"SELECT count(distinct subreddit) AS Unique_Subreddit from comments\")\n",
    "# sqlDF.show()\n",
    "\n",
    "\n",
    "\n",
    "# df = sqlContext.read.json(\"hdfs://orion11:41001/reddit/*\")\n",
    "# df.createOrReplaceTempView(\"Comments\")\n",
    "\n",
    "# Read from 2005 to 2017\n",
    "\n",
    "# sqlDF=spark.sql(\"SELECT subredit,count(*) AS CommentCount from Comments and subreddit='Earthquake' and (created_utc>=1104552009000 and created_utc<=1328054399)\")\n",
    "\n",
    "# sqlDF.show()\n",
    "# sqlDF.show(df.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The following table counts the number of comments in the subreddit between the years 2006 up to 2017. The actual number of earthquakes are also calculated based on Science for changing word dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|year|count(1)|\n",
      "+----+--------+\n",
      "|2017|      19|\n",
      "|2016|      95|\n",
      "|2015|      84|\n",
      "|2014|      59|\n",
      "|2013|      39|\n",
      "|2012|      40|\n",
      "|2011|      21|\n",
      "|2010|       8|\n",
      "|2009|       1|\n",
      "|2008|       2|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "earthquakeComment = filteredComment.filter(filteredComment.body.rlike('earthquake'))\n",
    "countDF = earthquakeComment.groupBy('year').agg(func.count(func.lit(1))).sort(func.desc('year'))\n",
    "countDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "countDF.coalesce(1).write.format('csv').save('hdfs://orion11:41001/A7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3228"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true').load('hdfs://orion11:41001/Earthquake/2017.csv')\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result: \n",
    "\n",
    "Although the number of comments were increased year by year except for 2017, however there is no strong correlation between subreddit comments and the number of actual earthquakes registered in the dataset. There may be the possibility that people used different platform rather than Reddit to talk about earthquake.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Q1 - Analysis](earthquake1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2) Use Machine Learning to  calculate the maximum number of earthquake in 2020?\n",
    "\n",
    "We have used the same dataset taken from the “science for changing world website” to train the system for predicting the number of earthquakes in 2020. In this model the linear regression model was considered. The data for 2006 till 2017 is considered to train the data.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.87 ms, sys: 970 µs, total: 4.84 ms\n",
      "Wall time: 8.24 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "    \n",
    "# Load data\n",
    "earthquake_file_path = '/home4/rteymourzadeh/Project3BigData/P3-p3-hiep_alper_team/A7_Final_Analysis/ML.csv'\n",
    "earthquake_data = pd.read_csv(earthquake_file_path) \n",
    "earthquake_data.columns\n",
    "\n",
    "# Choose target and features\n",
    "y = earthquake_data.Earthquake\n",
    "earthquake_features =['Year']\n",
    "\n",
    "X = earthquake_data[earthquake_features]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into training and validation data, for both features and target\n",
    "# The split is based on a random number generator. Supplying a numeric value to\n",
    "# the random_state argument guarantees we get the same split every time we\n",
    "# run this script.\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2012]\n",
      " [2017]\n",
      " [2010]\n",
      " [2018]\n",
      " [2019]\n",
      " [2020]]\n",
      "[2723.9537037  4098.16666667 2174.26851852 4373.00925926 4647.85185185\n",
      " 4922.69444444]\n",
      "CPU times: user 2.76 ms, sys: 0 ns, total: 2.76 ms\n",
      "Wall time: 1.74 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ML.csv (Year,earthquake)\n",
    "import numpy as np\n",
    "np_arr1 = np.array([[2018],[2019],[2020]])\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "#forest_model = RandomForestRegressor(n_estimators=100,random_state=1)\n",
    "forest_model = LinearRegression()\n",
    "\n",
    "forest_model.fit(train_X, train_y)\n",
    "#forest_model.fit(X,y)\n",
    "\n",
    "val_X=np.append(val_X,np_arr1,axis=0)  \n",
    "print(val_X)\n",
    "\n",
    "\n",
    "#val_X=val_X.insert([4],[2017])\n",
    "melb_preds = forest_model.predict(val_X)\n",
    "print(melb_preds)\n",
    "#print('Mean_absolute_error:',mean_absolute_error(val_y, melb_preds))\n",
    "#print(val_X.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result and Discussion\n",
    "\n",
    "We tried to calculate the number of earthquakes that may happened in 2018, 2019 and 2020. Those values are: \n",
    "\n",
    "Colons can be used to align columns.\n",
    "\n",
    "#### Trained Data:\n",
    "\n",
    "**No** | **Year** | **Earthquake**\n",
    "--- | --- | ---\n",
    "1 | 2018 | 4373\n",
    "2 | 2019 | 4647\n",
    "3 | 2020 | 4922\n",
    "\n",
    "Since we have a real data from the data set for 2018 and 2019 we can see the accuracy of the model: \n",
    "\n",
    "#### Real Data:\n",
    "\n",
    "**No** | **Year** | **Earthquake**\n",
    "--- | --- | ---\n",
    "1 | 2018 | 2135\n",
    "2 | 2019 | 4152\n",
    "3 | 2020 | ?\n",
    "\n",
    "\n",
    "\n",
    "![Q2 - Analysis](earthquake3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained result is shown in the above graph. It shows 4373 and 4647 earthquakes for the 2018 and 2019 respectively.  Although, the trend is increasing for the number of the earthquakes that looks correct prediction, yet still not accurate for 2018. The model can improve using more data to train the model or we can use classifier model instead of regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3) Use Machine Learning to calculate the strongest earthquick may happen in 2020?\n",
    "\n",
    "An earthquake radiates energy in the form of different kinds of seismic waves, whose characteristics reflect the nature of both the rupture and the earth's crust the waves travel through.Determination of an earthquake's magnitude generally involves identifying specific kinds of these waves on a seismogram, and then measuring one or more characteristics of a wave, such as its timing, orientation, amplitude, frequency, or duration. \n",
    "Additional adjustments are made for distance, kind of crust, and the characteristics of the seismograph that recorded the seismogram. Sample of magnitude scale is shown in the following figure.\n",
    "In this question, we have used the same dataset taken from the “science for changing world website” to train the system for predicting the earthquakes magnitude in 2020. In this model the linear regression model was considered. The data for 2006 till 2017 is considered to train the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 754 ms, sys: 111 ms, total: 865 ms\n",
      "Wall time: 6.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "    \n",
    "# Load data\n",
    "earthquake_file_path = '/home4/rteymourzadeh/Project3BigData/P3-p3-hiep_alper_team/A7_Final_Analysis/ML2.csv'\n",
    "earthquake_data = pd.read_csv(earthquake_file_path) \n",
    "earthquake_data.columns\n",
    "\n",
    "# Choose target and features\n",
    "y = earthquake_data.Earthquake\n",
    "earthquake_features =['Year']\n",
    "\n",
    "X = earthquake_data[earthquake_features]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# run this script.\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2012]\n",
      " [2017]\n",
      " [2010]\n",
      " [2018]\n",
      " [2019]\n",
      " [2020]]\n",
      "[5.87833333 5.57       6.00166667 5.50833333 5.44666667 5.385     ]\n",
      "CPU times: user 19.6 ms, sys: 7.89 ms, total: 27.5 ms\n",
      "Wall time: 518 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ML2.csv (Year,earthquake)\n",
    "import numpy as np\n",
    "np_arr1 = np.array([[2018],[2019],[2020]])\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "forest_model = LinearRegression()\n",
    "\n",
    "forest_model.fit(train_X, train_y)\n",
    "\n",
    "val_X=np.append(val_X,np_arr1,axis=0)  \n",
    "print(val_X)\n",
    "\n",
    "melb_preds = forest_model.predict(val_X)\n",
    "print(melb_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result and Discussion\n",
    "\n",
    "We tried to calculate the number of earthquakes that may happened in 2018, 2019 and 2020. Those values are: \n",
    "\n",
    "Colons can be used to align columns.\n",
    "\n",
    "#### Trained Data:\n",
    "\n",
    "**No** | **Year** | **Earthquake**\n",
    "--- | --- | ---\n",
    "1 | 2018 | 5.5\n",
    "2 | 2019 | 5.4\n",
    "3 | 2020 | 5.3\n",
    "\n",
    "Since we have a real data from the data set for 2018 and 2019 we can see the accuracy of the model: \n",
    "\n",
    "#### Real Data:\n",
    "\n",
    "**No** | **Year** | **Earthquake**\n",
    "--- | --- | ---\n",
    "1 | 2018 | 5.29\n",
    "2 | 2019 | 7.1\n",
    "3 | 2020 | ?\n",
    "\n",
    "\n",
    "\n",
    "![Q3 - Analysis](earthquake5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained result is shown in the above graph. It shows 5.5 and 5.4 and 5.3 magnitude earthquakes for the respective year of 2018 , 2019 and 2020. Although, the trend shows decreasing value for the magnitude of the earthquakes that looks correct prediction, yet still not accurate for 2019. The model can improve using more data to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "[1] Dataset: [USGS](https://earthquake.usgs.gov/earthquakes/search/) <br/>\n",
    "[2] Earthquake Wiki: [Earthquake](https://en.wikipedia.org/wiki/Seismic_magnitude_scales)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
