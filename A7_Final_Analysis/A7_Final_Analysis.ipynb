{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis-7 Final Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve had the opportunity to analyze two datasets thus far; now it’s time to analyze a dataset of your own. Find a dataset online and use Spark (or Hadoop) to analyze it. You should:\n",
    "\n",
    "1. Describe the dataset (Earthquack)\n",
    "2. Outline the types of insights you hope to gain from it\n",
    "3. Make hypotheses about what you might find\n",
    "4. Design at least 3 “questions” (along the lines of those above) and answer them. Remember that presentation matters here. ML Models are a good choice for some of the datasets; you can describe what you’ll try to predict or classify and outline your experiences with various models.\n",
    "\n",
    "Q1) Howmany people talk in subreddit earthquake and the correlation on a real earthquake\n",
    "\n",
    "Q2) 10 years get data from Earthquake web site. 1990=2310, ,  2006=190, 2007=200, 2008=230  , 2021=Make Prediction.\n",
    "\n",
    "Q3) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction About Upcoming New Reddit User Numbers (Apply ML Model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the given data set, we will find the unique user numbers in reddit for each month and then we will make prediction for upcoming month's unique user numbers. (We will use ML models) By doing this reddit development team can prepare themselves for upcoming load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 ms, sys: 5.02 ms, total: 10 ms\n",
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark.catalog.dropGlobalTempView(\"Comments\")\n",
    "df = sqlContext.read.json(\"hdfs://orion11:11001/sampled_reddit_v3/*\")\n",
    "df.createGlobalTempView(\"Comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2742841\n",
      "CPU times: user 11.7 ms, sys: 2.55 ms, total: 14.2 ms\n",
      "Wall time: 34.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "botExpr = \"[bB][oO][tT]\"\n",
    "\n",
    "filteredComment = (df\n",
    "                   .filter(~(df.body.like(\"[deleted]\") \n",
    "                             | df.body.like('[removed]') \n",
    "                             | df.author.rlike(botExpr) \n",
    "                             | df.author.like(\"[deleted]\")\n",
    "                            )\n",
    "                          )\n",
    "                  )\n",
    "df.unpersist()\n",
    "filteredComment.cache()\n",
    "print(filteredComment.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add month and year column into the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.4 ms, sys: 3.09 ms, total: 16.5 ms\n",
      "Wall time: 1.19 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2742841"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql.functions import year, month, dayofmonth, from_unixtime\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "filteredComment = (filteredComment\n",
    "      .withColumn(\"year\", year(from_unixtime(\"created_utc\").cast(DateType())))\n",
    "      .withColumn(\"month\",month(from_unixtime(\"created_utc\").cast(DateType()))))\n",
    "filteredComment.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1) Correlation between people comment in subreddit about earthquak and actual earthquake per year: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have targetted the US regien as shown bellow and use the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.73 ms, sys: 77 µs, total: 3.81 ms\n",
      "Wall time: 3.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "originDF = sqlContext.read.json(\"hdfs://orion11:11001/sampled_reddit_v3/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------+\n",
      "|year|           subreddit|count(1)|\n",
      "+----+--------------------+--------+\n",
      "|2014|          skyrimmods|     136|\n",
      "|2014|                deaf|       8|\n",
      "|2014|               dubai|      30|\n",
      "|2014|                lost|      20|\n",
      "|2014|        raspberry_pi|      59|\n",
      "|2014|terriblefacebookm...|      16|\n",
      "|2014|       ClickerHeroes|      89|\n",
      "|2014|            GameSale|      52|\n",
      "|2014|RandomActsOfChris...|      14|\n",
      "|2014|         RateMyMayor|      90|\n",
      "|2014|              Flyers|      94|\n",
      "|2014|          EmeraldPS2|      19|\n",
      "|2014|     douglovesmovies|       5|\n",
      "|2015|               trees|    2034|\n",
      "|2015|      TumblrInAction|    1815|\n",
      "|2014|          glassheads|      30|\n",
      "|2015|           firewater|       6|\n",
      "|2015|            formula1|     842|\n",
      "|2014|              ukguns|       5|\n",
      "|2015|              Nissan|      10|\n",
      "+----+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+--------------+\n",
      "|year|NumofSubReddit|\n",
      "+----+--------------+\n",
      "+----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SAMPLED Data\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "#df = sqlContext.read.json(\"hdfs://orion11:41001/sampled_reddit/*\")\n",
    "#df.createOrReplaceTempView(\"comments\")\n",
    "# df.saveAsTable(\"Unique subreddit comments\")\n",
    "\n",
    "# subredditDF = filteredComment.select(\"subreddit\").distinct()\n",
    "\n",
    "subredditDf = filteredComment.groupBy('year','subreddit').agg(func.count(func.lit(1)))\n",
    "subredditDf.show()\n",
    "subredditDf = (subredditDf\n",
    "               .filter(subredditDf.subreddit.rlike('earthquake'))\n",
    "               .groupBy('year')\n",
    "               .agg(func.count(func.lit(1)).alias('NumofSubReddit')))\n",
    "\n",
    "subredditDf.show()\n",
    "\n",
    "\n",
    "# subredditDF = SubrredditDF.filterBy('subredd')(subreddit\n",
    "#                         .groupBy('year')\n",
    "#                         .agg(func.count('Earthquake').alias('numOfComments')))\n",
    "\n",
    "\n",
    "# sqlDF=spark.sql(\"SELECT count(distinct subreddit) AS Unique_Subreddit from comments\")\n",
    "# sqlDF.show()\n",
    "\n",
    "\n",
    "\n",
    "# df = sqlContext.read.json(\"hdfs://orion11:41001/reddit/*\")\n",
    "# df.createOrReplaceTempView(\"Comments\")\n",
    "\n",
    "# Read from 2005 to 2017\n",
    "\n",
    "# sqlDF=spark.sql(\"SELECT subredit,count(*) AS CommentCount from Comments and subreddit='Earthquake' and (created_utc>=1104552009000 and created_utc<=1328054399)\")\n",
    "\n",
    "# sqlDF.show()\n",
    "# sqlDF.show(df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|year|count(1)|\n",
      "+----+--------+\n",
      "|2017|      19|\n",
      "|2016|      95|\n",
      "|2015|      84|\n",
      "|2014|      59|\n",
      "|2013|      39|\n",
      "|2012|      40|\n",
      "|2011|      21|\n",
      "|2010|       8|\n",
      "|2009|       1|\n",
      "|2008|       2|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "earthquakeComment = filteredComment.filter(filteredComment.body.rlike('earthquake'))\n",
    "countDF = earthquakeComment.groupBy('year').agg(func.count(func.lit(1))).sort(func.desc('year'))\n",
    "countDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "countDF.coalesce(1).write.format('csv').save('hdfs://orion11:41001/A7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3228"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true').load('hdfs://orion11:41001/Earthquake/2017.csv')\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Q1 - Analysis](earthquake1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.87 ms, sys: 970 µs, total: 4.84 ms\n",
      "Wall time: 8.24 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "    \n",
    "# Load data\n",
    "earthquake_file_path = '/home4/rteymourzadeh/Project3BigData/P3-p3-hiep_alper_team/A7_Final_Analysis/ML.csv'\n",
    "earthquake_data = pd.read_csv(earthquake_file_path) \n",
    "earthquake_data.columns\n",
    "\n",
    "# Choose target and features\n",
    "y = earthquake_data.Earthquake\n",
    "earthquake_features =['Year']\n",
    "\n",
    "X = earthquake_data[earthquake_features]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into training and validation data, for both features and target\n",
    "# The split is based on a random number generator. Supplying a numeric value to\n",
    "# the random_state argument guarantees we get the same split every time we\n",
    "# run this script.\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2012]\n",
      " [2017]\n",
      " [2010]\n",
      " [2018]\n",
      " [2019]\n",
      " [2020]]\n",
      "[2723.9537037  4098.16666667 2174.26851852 4373.00925926 4647.85185185\n",
      " 4922.69444444]\n",
      "CPU times: user 2.76 ms, sys: 0 ns, total: 2.76 ms\n",
      "Wall time: 1.74 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ML.csv (Year,earthquake)\n",
    "import numpy as np\n",
    "np_arr1 = np.array([[2018],[2019],[2020]])\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "#forest_model = RandomForestRegressor(n_estimators=100,random_state=1)\n",
    "forest_model = LinearRegression()\n",
    "\n",
    "forest_model.fit(train_X, train_y)\n",
    "#forest_model.fit(X,y)\n",
    "\n",
    "val_X=np.append(val_X,np_arr1,axis=0)  \n",
    "print(val_X)\n",
    "\n",
    "\n",
    "#val_X=val_X.insert([4],[2017])\n",
    "melb_preds = forest_model.predict(val_X)\n",
    "print(melb_preds)\n",
    "#print('Mean_absolute_error:',mean_absolute_error(val_y, melb_preds))\n",
    "#print(val_X.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result and Discussion\n",
    "\n",
    "We tried to calculate the number of earthquakes that may happened in 2018, 2019 and 2020. Those values are: \n",
    "\n",
    "Colons can be used to align columns.\n",
    "\n",
    "#### Trained Data:\n",
    "\n",
    "**No** | **Year** | **Earthquake**\n",
    "--- | --- | ---\n",
    "1 | 2018 | 4373\n",
    "2 | 2019 | 4647\n",
    "3 | 2020 | 4922\n",
    "\n",
    "Since we have a real data from the data set for 2018 and 2019 we can see the accuracy of the model: \n",
    "\n",
    "#### Real Data:\n",
    "\n",
    "**No** | **Year** | **Earthquake**\n",
    "--- | --- | ---\n",
    "1 | 2018 | 2135\n",
    "2 | 2019 | 4152\n",
    "3 | 2020 | ?\n",
    "\n",
    "\n",
    "\n",
    "![Q2 - Analysis](earthquake3.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
