{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import StringType, DoubleType, ArrayType\n",
    "import string\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = spark.read.format('json').load('hdfs://orion11:11001/reddit/*/*')\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleDF = df.sample(False, 0.1)\n",
    "sampleDF.write.format('json').save('hdfs://orion11:11001/sampled_reddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309199315\n",
      "CPU times: user 82.6 ms, sys: 41.7 ms, total: 124 ms\n",
      "Wall time: 7min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sampleDF = sqlContext.read.json(\"hdfs://orion11:11001/sampled_reddit/*\")\n",
    "sampleDF.cache()\n",
    "print(sampleDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleDFv2 = sampleDF.sample(False, 0.1)\n",
    "sampleDFv2.write.format('json').save('hdfs://orion11:11001/sampled_reddit_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30907764\n",
      "CPU times: user 14.3 ms, sys: 6.51 ms, total: 20.9 ms\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sampleDFv2 = spark.read.format('json').load('hdfs://orion11:11001/sampled_reddit_v2/*')\n",
    "sampleDFv2.cache()\n",
    "print(sampleDFv2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleDFv3 = sampleDFv2.sample(False, 0.1)\n",
    "sampleDFv3.write.format('json').save('hdfs://orion11:11001/sampled_reddit_v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3090865\n",
      "CPU times: user 3.01 ms, sys: 2.14 ms, total: 5.15 ms\n",
      "Wall time: 13.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sampleDFv3 = spark.read.format('json').load('hdfs://orion11:11001/sampled_reddit_v3/*')\n",
    "sampleDFv3.cache()\n",
    "print(sampleDFv3.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter all comment generate by bot/ comment that is delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286117107\n",
      "CPU times: user 48.6 ms, sys: 27.5 ms, total: 76.1 ms\n",
      "Wall time: 4min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "botExpr = \"[bB][oO][tT]\"\n",
    "\n",
    "filteredComment = (sampleDF\n",
    "                   .filter(~(sampleDF.body.like(\"[deleted]\") \n",
    "                             | sampleDF.body.like('[removed]') \n",
    "                             | sampleDF.author.rlike(botExpr) \n",
    "                             | sampleDF.author.like(\"[deleted]\")\n",
    "                            )\n",
    "                          )\n",
    "                  )\n",
    "sampleDF.unpersist()\n",
    "filteredComment.cache()\n",
    "print(filteredComment.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look up for comment that might contain the Countries name\n",
    "To guess that if the comment may contain the countries, we will check if the comment contain the substring __*m from*__ or __*come from*__ for example:   \n",
    "* \"I'm from Vietnam\"  \n",
    "* \"I come from Turkey\"  \n",
    "Then we will check the word next to the keyword to see if it is the country name by using __pycountry__ library. If it is a country, we will add to the set to make sure that we will not generate duplicate for that user.  \n",
    "After that, we only select the user that introduce about their contry and keep it in dataframe to use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pycountry\n",
    "\n",
    "def preProcessBody(text):\n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    # remove special characters and digits\n",
    "#     text=re.sub(\"(\\\\d|[^\\\\w|\\\\s]|(\\_))+\",\"\",text)\n",
    "#     text=re.sub(\"(\\\\s)+\",\" \",text)\n",
    "    text=re.sub(\"[^A-Za-z]+\",\" \",text)\n",
    "    return text.strip()\n",
    "\n",
    "def getCountryKeyWord(text):\n",
    "    text = preProcessBody(text)\n",
    "    pattern = re.compile('(m from)|(come from)')\n",
    "#     s = \"I'm from Belgium I'll insult that farce for a sport as many names as I damn well please. I'm from Vietnam\"\n",
    "    # l = re.compile(\"(?<!^)\\s+(?=[A-Z])(?!.\\s)\").split(s)\n",
    "    subString = pattern.split(text)\n",
    "    subString = list(filter(None, subString))\n",
    "#     print(subString)\n",
    "    numSubString = len(subString)\n",
    "    result = set([])\n",
    "    for i in range(numSubString-1):\n",
    "#         print(subString[i+1])\n",
    "        if pattern.match(subString[i]):\n",
    "            words = subString[i+1].split()\n",
    "            if len(words)>0:\n",
    "                firstWord = subString[i+1].split()[0]\n",
    "                name = pycountry.countries.get(name=firstWord.capitalize())\n",
    "                officialName = pycountry.countries.get(official_name=firstWord.capitalize())\n",
    "                if name or officialName:\n",
    "                    result.add(firstWord)\n",
    "    return list(result)\n",
    "            \n",
    "getCountryKeyWordUdf = func.udf(getCountryKeyWord, ArrayType(StringType()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.09 ms, sys: 222 Âµs, total: 4.31 ms\n",
      "Wall time: 36.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "authorCountries = ((filteredComment.withColumn(\"countries\", getCountryKeyWordUdf(filteredComment.body)))\n",
    " .filter(func.size(func.col('countries'))>0)\n",
    " .select('author','countries'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeCountries(countriesList):\n",
    "    result = set([])\n",
    "    for countries in countriesList:\n",
    "        for country in countries:\n",
    "            result.add(country);\n",
    "    return list(result)\n",
    "\n",
    "mergeCountriesUdf = func.udf(mergeCountries, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------------------------------------+\n",
      "|           author|mergeCountries(collect_list(countries, 0, 0))|\n",
      "+-----------------+---------------------------------------------+\n",
      "|            24man|                                     [norway]|\n",
      "|        AdowTatep|                                     [brazil]|\n",
      "|         Aggron82|                                     [kuwait]|\n",
      "|         Anatummy|                                     [france]|\n",
      "|    AnnoyingSwede|                                     [sweden]|\n",
      "|        Astarmoth|                                     [mexico]|\n",
      "|      Bart_olomeo|                                    [belgium]|\n",
      "|        Bennators|                                     [canada]|\n",
      "|      BerntBrakar|                                     [norway]|\n",
      "| BetweenTheLayers|                                  [singapore]|\n",
      "| BlueBarracudaBro|                                     [canada]|\n",
      "|         Bohne_13|                                    [germany]|\n",
      "|     BorussiaMG93|                                    [germany]|\n",
      "|        Bumba1661|                                    [finland]|\n",
      "|           Buscat|                             [canada, turkey]|\n",
      "|CharredOldOakCask|                                     [norway]|\n",
      "|      Clivepwnens|                                 [antarctica]|\n",
      "|        Cognosis5|                                  [australia]|\n",
      "|       CookieCash|                                    [germany]|\n",
      "|     DanielTaylor|                                      [spain]|\n",
      "+-----------------+---------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "authorCountries = (authorCountries.groupBy('author')\n",
    "                   .agg(mergeCountriesUdf(func.collect_list(authorCountries.countries))))\n",
    "authorCountries.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "472"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filteredComment.filter(filteredComment.author == 'spoleto').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i ', 'm from', ' belgium i ll insult that farce for a sport as many names as i damn well please i ', 'm from', ' vietnam']\n",
      "m from\n",
      " belgium i ll insult that farce for a sport as many names as i damn well please i \n",
      "m from\n",
      " vietnam\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['belgium']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getCountryKeyWord(\"I'm from Belgium I'll insult that farce for a sport as many names as I damn well please. I'm from Vietnam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate setiment score for an author at all subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # be sure to have stopwords installed for this using nltk.download_shell()\n",
    "import string\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql import types as types\n",
    "\n",
    "# This Cell takes 4 minutes... (199923 records for soccer)\n",
    "nltk.downloader.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# install Vader and make sure you download the lexicon as well\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def calculatue_score(listBody):    \n",
    "    #0.1 sample\n",
    "    #newsamp = sqlDF.filter(sqlDF.subreddit.like(category)&~sqlDF['author'].isin(['[deleted]']))\n",
    "    \n",
    "    #0.01 sample\n",
    "    #newsamp = originDF.filter(originDF.subreddit.like(category)&~originDF['author'].isin(['[deleted]']))\n",
    "#     newsamp = originDF.filter(originDF['subreddit'].isin(category)&~originDF['author'].isin(['[deleted]']))    \n",
    "#     iteratebody = newsamp.select(\"body\").rdd.flatMap(list).collect()    \n",
    "    \n",
    "    if(len(listBody)>100):                            \n",
    "        # this step will return an error if you have not installed the lexicon \n",
    "        result = 0.0;\n",
    "        for message in listBody:   \n",
    "            #clean the comments\n",
    "            message = pre_process(message)\n",
    "\n",
    "            ss = sid.polarity_scores(message)\n",
    "            result += ss[\"compound\"]\n",
    "\n",
    "        #print(summary)\n",
    "        return result\n",
    "    else:\n",
    "        #print('Sorry less than 100 comments.')\n",
    "        return 0\n",
    "    \n",
    "calculatue_score_udf = func.udf(calculatue_score, types.FloatType())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
