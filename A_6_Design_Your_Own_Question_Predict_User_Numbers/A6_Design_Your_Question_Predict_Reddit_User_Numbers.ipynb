{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis-6 Design Your Own Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that youâ€™ve found the answers to the questions above, design two of your own questions to answer. These should be sufficiently difficult, and you should be creative! You should start with a question, and then propose a predicted answer or hypothesis before writing a MapReduce job to answer it. If you come up with a particularly challenging question, it can count for two (ask first)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction About Upcoming New Reddit User Numbers (Apply ML Model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the given data set, we will find the unique user numbers in reddit for each month and then we will make prediction for upcoming month's unique user numbers. (We will use ML models) By doing this reddit development team can prepare themselves for upcoming load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.32 ms, sys: 1.59 ms, total: 9.92 ms\n",
      "Wall time: 25.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark.catalog.dropGlobalTempView(\"Comments\")\n",
    "df = sqlContext.read.json(\"hdfs://orion11:11001/sampled_reddit_v2/*\")\n",
    "df.createGlobalTempView(\"Comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27429190\n",
      "CPU times: user 5.89 ms, sys: 4.7 ms, total: 10.6 ms\n",
      "Wall time: 26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "botExpr = \"[bB][oO][tT]\"\n",
    "\n",
    "filteredComment = (df\n",
    "                   .filter(~(df.body.like(\"[deleted]\") \n",
    "                             | df.body.like('[removed]') \n",
    "                             | df.author.rlike(botExpr) \n",
    "                             | df.author.like(\"[deleted]\")\n",
    "                            )\n",
    "                          )\n",
    "                  )\n",
    "df.unpersist()\n",
    "filteredComment.cache()\n",
    "print(filteredComment.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add month and year column into the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.35 ms, sys: 2.16 ms, total: 4.51 ms\n",
      "Wall time: 903 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27429190"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql.functions import year, month, dayofmonth, from_unixtime\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "filteredComment = (filteredComment\n",
    "      .withColumn(\"year\", year(from_unixtime(\"created_utc\").cast(DateType())))\n",
    "      .withColumn(\"month\",month(from_unixtime(\"created_utc\").cast(DateType()))))\n",
    "filteredComment.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of active user for each month and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[month: int, year: int, NumOfUser: bigint]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "userByTime = filteredComment.groupBy('month','year').agg(countDistinct('author').alias('NumOfUser'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---------+\n",
      "|month|year|NumOfUser|\n",
      "+-----+----+---------+\n",
      "|   12|2005|        7|\n",
      "|    1|2006|       29|\n",
      "|    2|2006|       58|\n",
      "|    3|2006|       83|\n",
      "|    4|2006|      129|\n",
      "|    5|2006|      156|\n",
      "|    6|2006|      173|\n",
      "|    7|2006|      212|\n",
      "|    8|2006|      310|\n",
      "|    9|2006|      280|\n",
      "|   10|2006|      325|\n",
      "|   11|2006|      355|\n",
      "|   12|2006|      393|\n",
      "|    1|2007|      461|\n",
      "|    2|2007|      538|\n",
      "|    3|2007|      632|\n",
      "|    4|2007|      671|\n",
      "|    5|2007|      819|\n",
      "|    6|2007|      830|\n",
      "|    7|2007|      984|\n",
      "+-----+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as func\n",
    "userByTime = userByTime.sort(func.asc('year'), func.asc('month'))\n",
    "userByTime.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "userByTime.coalesce(1).write.format('csv').save('hdfs://orion11:11001/A6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
