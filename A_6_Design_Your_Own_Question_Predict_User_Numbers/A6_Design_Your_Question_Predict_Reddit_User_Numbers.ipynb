{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis-6 Design Your Own Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you’ve found the answers to the questions above, design two of your own questions to answer. These should be sufficiently difficult, and you should be creative! You should start with a question, and then propose a predicted answer or hypothesis before writing a MapReduce job to answer it. If you come up with a particularly challenging question, it can count for two (ask first)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction About Upcoming New Reddit User Numbers (Apply ML Model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the given data set, we will find the unique user numbers in reddit for each month and then we will make prediction for upcoming month's unique user numbers. (We will use ML models) By doing this reddit development team can prepare themselves for upcoming load.\n",
    "\n",
    "First things first. Lets calculate Reddit User Numbers for each month in the past. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.4 ms, sys: 7.9 ms, total: 32.3 ms\n",
      "Wall time: 4min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark.catalog.dropGlobalTempView(\"Comments\")\n",
    "#df = sqlContext.read.json(\"hdfs://orion11:11001/sampled_reddit_v2/*\")\n",
    "df = sqlContext.read.json(\"hdfs://orion11:11001/sampled_reddit/*\")\n",
    "df.createGlobalTempView(\"Comments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filtered uncessary comments and cleaned our data as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274394659\n",
      "CPU times: user 17.6 ms, sys: 6.28 ms, total: 23.9 ms\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "botExpr = \"[bB][oO][tT]\"\n",
    "\n",
    "filteredComment = (df\n",
    "                   .filter(~(df.body.like(\"[deleted]\") \n",
    "                             | df.body.like('[removed]') \n",
    "                             | df.author.rlike(botExpr) \n",
    "                             | df.author.like(\"[deleted]\")\n",
    "                            )\n",
    "                          )\n",
    "                  )\n",
    "df.unpersist()\n",
    "filteredComment.cache()\n",
    "print(filteredComment.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add month and year column into the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.3 ms, sys: 18.3 ms, total: 75.7 ms\n",
      "Wall time: 1min 22s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "274394659"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql.functions import year, month, dayofmonth, from_unixtime\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "filteredComment = (filteredComment\n",
    "      .withColumn(\"year\", year(from_unixtime(\"created_utc\").cast(DateType())))\n",
    "      .withColumn(\"month\",month(from_unixtime(\"created_utc\").cast(DateType()))))\n",
    "filteredComment.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of active user for each month and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.79 ms, sys: 1.11 ms, total: 5.9 ms\n",
      "Wall time: 139 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "userByTime = filteredComment.groupBy('month','year').agg(countDistinct('author').alias('NumOfUser'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---------+\n",
      "|month|year|NumOfUser|\n",
      "+-----+----+---------+\n",
      "|   12|2005|       68|\n",
      "|    1|2006|      196|\n",
      "|    2|2006|      416|\n",
      "|    3|2006|      532|\n",
      "|    4|2006|      668|\n",
      "|    5|2006|      839|\n",
      "|    6|2006|      858|\n",
      "|    7|2006|     1097|\n",
      "|    8|2006|     1414|\n",
      "|    9|2006|     1449|\n",
      "|   10|2006|     1606|\n",
      "|   11|2006|     1844|\n",
      "|   12|2006|     1947|\n",
      "|    1|2007|     2263|\n",
      "|    2|2007|     2521|\n",
      "|    3|2007|     2986|\n",
      "|    4|2007|     3038|\n",
      "|    5|2007|     3621|\n",
      "|    6|2007|     3690|\n",
      "|    7|2007|     4051|\n",
      "+-----+----+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 13.7 ms, sys: 6.84 ms, total: 20.6 ms\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pyspark.sql.functions as func\n",
    "userByTime = userByTime.sort(func.asc('year'), func.asc('month'))\n",
    "userByTime.show()\n",
    "\n",
    "#for row in userByTime.rdd.collect():\n",
    "#    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "userByTime.coalesce(1).write.format('csv').save('hdfs://orion11:31001/A6_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Existing User Numbers from 2005 to 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of our analysis, we calculated the Reddit User Numbers from 2005 to 2007.\n",
    "\n",
    "![alt text](https://i.imgur.com/8gC0qtm.jpg \"Logo Title Text 1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Machine Learning Model (Linear Regression Model)\n",
    "\n",
    "Based on the graph above, we decided to use Linear Regression ML Model. We eliminated the first 55 months to be able make a better prediction. Because for first 55 months user numbers are relatively very low as you seen in the above graph. \n",
    "We fit our ML.csv data into LinearRegression model. \n",
    "\n",
    "Linear Regression is a machine learning algorithm based on supervised learning. It performs a regression task. Regression models a target prediction value based on independent variables. It is mostly used for finding out the relationship between variables and forecasting. Different regression models differ based on – the kind of relationship between dependent and independent variables, they are considering and the number of independent variables being used. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.96 ms, sys: 8.34 ms, total: 16.3 ms\n",
      "Wall time: 1.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "    \n",
    "# Load data\n",
    "melbourne_file_path = '/home4/saozdamar/ML.csv'\n",
    "melbourne_data = pd.read_csv(melbourne_file_path) \n",
    "# Filter rows with missing values\n",
    "#melbourne_data = melbourne_data.dropna(axis=0)\n",
    "# Choose target and features\n",
    "y = melbourne_data.Number\n",
    "melbourne_features = ['Month','Year']\n",
    "#melbourne_features = ['RL']\n",
    "X = melbourne_data[melbourne_features]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into training and validation data, for both features and target\n",
    "# The split is based on a random number generator. Supplying a numeric value to\n",
    "# the random_state argument guarantees we get the same split every time we\n",
    "# run this script.\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we fit our current data into linear regression model. In the below, we are passing our desired month and year value. (month=1 January, month=2 February ,so on...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   5 2012]\n",
      " [  10 2012]\n",
      " [   8 2015]\n",
      " [   8 2011]\n",
      " [   6 2016]\n",
      " [   9 2016]\n",
      " [   1 2013]\n",
      " [   2 2015]\n",
      " [  12 2014]\n",
      " [   9 2012]\n",
      " [   9 2014]\n",
      " [   1 2014]\n",
      " [   7 2014]\n",
      " [   4 2013]\n",
      " [   8 2016]\n",
      " [   9 2010]\n",
      " [   4 2015]\n",
      " [   1 2016]\n",
      " [  12 2013]\n",
      " [  10 2010]\n",
      " [   4 2016]\n",
      " [   8 2017]\n",
      " [   9 2017]\n",
      " [   5 2020]]\n",
      "[ 420218.81944352  511173.22785372 1178645.96654123  240173.29713911\n",
      " 1376882.37052768 1431455.0155738   582073.46006596 1069500.67644906\n",
      " 1016791.32591879  492982.34617168  962218.68087274  816691.62741649\n",
      "  925836.91750866  636646.10511202 1413264.13389176   23746.01147062\n",
      " 1105882.43981308 1285927.96211755  782173.15856826   41936.89315265\n",
      " 1340500.60716361 1647882.30124229 1666073.18292433 2297164.15824777]\n",
      "CPU times: user 5.78 ms, sys: 5.57 ms, total: 11.4 ms\n",
      "Wall time: 823 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ML.csv (month,year,userNumber)\n",
    "import numpy as np\n",
    "np_arr1 = np.array([[8, 2017],[9, 2017],[5, 2020]])\n",
    "#np_arr1 = np.array([[137],[200]])\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "#forest_model = RandomForestRegressor(n_estimators=100,random_state=1)\n",
    "forest_model = LinearRegression()\n",
    "\n",
    "forest_model.fit(train_X, train_y)\n",
    "#forest_model.fit(X,y)\n",
    "\n",
    "val_X=np.append(val_X,np_arr1,axis=0)  \n",
    "print(val_X)\n",
    "\n",
    "\n",
    "#val_X=val_X.insert([4],[2017])\n",
    "melb_preds = forest_model.predict(val_X)\n",
    "print(melb_preds)\n",
    "#print('Mean_absolute_error:',mean_absolute_error(val_y, melb_preds))\n",
    "#print(val_X.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see our predictions for user numbers in next 6, 7 months and for May 2020. Expectied Reddit user numbers are : \n",
    "\n",
    "After 6 months-Sep 2017 : 1647882 \n",
    "\n",
    "After 7 months-Sep 2017 : 1666073\n",
    "\n",
    "After 3 years -May 2020 : 2297164 \n",
    "\n",
    "VERY IMPORTANT NOTE: We used %1 sampled data. So we need to multiply these predictions by 100. So actual predictions should be like this: \n",
    "\n",
    "After 6 months-Sep 2017 : 164788200 \n",
    "\n",
    "After 7 months-Sep 2017 : 166607300\n",
    "\n",
    "After 3 years -May 2020 : 229716400 \n",
    "\n",
    "\n",
    "![alt text](https://i.imgur.com/rHefKM2.jpg \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-) https://www.kaggle.com/dansbecker/random-forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
