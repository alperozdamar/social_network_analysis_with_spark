{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Question-4 Toxicity (Sentiment Analysis)\n",
    "\n",
    "Toxicity: using Sentiment Analysis, determine the top 5 positive subreddits and top 5 negative subreddits based on comment sentiment.\n",
    "\n",
    "Choose two subreddits focused on similar topics but with different views, e.g., /r/apple and /r/android. Compare the toxicity of the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://i.imgur.com/MijPBwS.jpg \"Logo Title Text 1\")\n",
    "\n",
    "Sentiment analysis is the automatic process of identifying positive, negative and neutral emotions in text. Equipped with AI, sentiment analysis allows businesses to understand how customers feel about their products and services and extract valuable insights that lead to better decision-making.\n",
    "\n",
    "In a world where we generate 2.5 quintillion bytes of data every day, sentiment analysis has become a key tool for making sense of that data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sampled original data to 1% for first part of this question. 1% data is enough to determine the top 5 positive subreddits and top 5 negative subreddits based on comment sentiment. Important Note: If there are less than 100 comments of a specific subreddit we didn't consider it in this analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.42 ms, sys: 2.19 ms, total: 4.61 ms\n",
      "Wall time: 9.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "originDF = sqlContext.read.json(\"hdfs://orion11:11001/sampled_reddit_v3/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the comment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def pre_process(text):\n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|[^\\\\w|\\\\s]|(\\_))+\",\"\",text)\n",
    "    text=re.sub(\"(\\\\s)+\",\" \",text)\n",
    "    #print(text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used NLTK /VADER library for Sentiment Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important Note: If there are less than 100 comments of a specific subreddit we didn't consider it in this analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home4/hpbui/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk # be sure to have stopwords installed for this using nltk.download_shell()\n",
    "import string\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql import types as types\n",
    "\n",
    "# This Cell takes 4 minutes... (199923 records for soccer)\n",
    "nltk.downloader.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# install Vader and make sure you download the lexicon as well\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def calculatue_score(listBody):    \n",
    "    #0.1 sample\n",
    "    #newsamp = sqlDF.filter(sqlDF.subreddit.like(category)&~sqlDF['author'].isin(['[deleted]']))\n",
    "    \n",
    "    #0.01 sample\n",
    "    #newsamp = originDF.filter(originDF.subreddit.like(category)&~originDF['author'].isin(['[deleted]']))\n",
    "#     newsamp = originDF.filter(originDF['subreddit'].isin(category)&~originDF['author'].isin(['[deleted]']))    \n",
    "#     iteratebody = newsamp.select(\"body\").rdd.flatMap(list).collect()    \n",
    "    \n",
    "    if(len(listBody)>100):                            \n",
    "        # this step will return an error if you have not installed the lexicon \n",
    "        result = 0.0;\n",
    "        for message in listBody:   \n",
    "            #clean the comments\n",
    "            message = pre_process(message)\n",
    "\n",
    "            ss = sid.polarity_scores(message)\n",
    "            result += ss[\"compound\"]\n",
    "\n",
    "        #print(summary)\n",
    "        return result\n",
    "    else:\n",
    "        #print('Sorry less than 100 comments.')\n",
    "        return 0\n",
    "    \n",
    "calculatue_score_udf = func.udf(calculatue_score, types.FloatType())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing sentiment analysis for each subreddit category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-----+\n",
      "|          subreddit|  analyze|count|\n",
      "+-------------------+---------+-----+\n",
      "|            Amateur|  24.4157|  153|\n",
      "|              HPMOR|  22.6543|  162|\n",
      "|         MLBTheShow|  63.3394|  380|\n",
      "|         MensRights|-197.9045| 3299|\n",
      "|         NHLStreams|  28.2473|  171|\n",
      "|         QuotesPorn|  32.2833|  289|\n",
      "|       SaltLakeCity|  41.0967|  280|\n",
      "|UnresolvedMysteries| -21.0719|  461|\n",
      "|         WahoosTipi|  27.2657|  257|\n",
      "|              anime| 1483.797| 9240|\n",
      "|           lacrosse|  27.2878|  110|\n",
      "|       marvelheroes|  67.5551|  320|\n",
      "|         mistyfront|   0.3322|  131|\n",
      "|             travel| 451.8471| 1454|\n",
      "|            ukraina|  16.0944|  655|\n",
      "| Anarcho_Capitalism| 120.7049| 1526|\n",
      "|          BABYMETAL| 108.9668|  390|\n",
      "|      ClickerHeroes|  90.6882|  357|\n",
      "|             Hawaii|  45.1344|  237|\n",
      "|          JUSTNOMIL|  70.3518|  735|\n",
      "+-------------------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 33.2 ms, sys: 7.26 ms, total: 40.5 ms\n",
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "analyzeDF = (originDF.groupBy('subreddit')\n",
    ".agg(calculatue_score_udf(F.collect_list('body')).alias('analyze'), F.count(F.lit(1)).alias('count')))\n",
    "analyzeDF = analyzeDF.filter(analyzeDF['count']>=100)\n",
    "analyzeDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(subreddit,StringType,true),StructField(analyze,FloatType,true),StructField(count,LongType,false)))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzeDF.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+------+\n",
      "|           subreddit|  analyze| count|\n",
      "+--------------------+---------+------+\n",
      "|           AskReddit|16001.021|284960|\n",
      "|     leagueoflegends| 5536.479| 48008|\n",
      "|              gaming|3953.6223| 41689|\n",
      "|                pics|3703.8025| 60123|\n",
      "|            gonewild|3235.5254| 13058|\n",
      "|                IAmA| 3080.609| 21385|\n",
      "|               funny|2940.4287| 64062|\n",
      "|                 nfl|2826.1062| 32719|\n",
      "|        pcmasterrace|2776.1965| 18199|\n",
      "|                 nba| 2721.648| 26823|\n",
      "|              soccer| 2531.825| 22251|\n",
      "|               trees|2440.6042| 17487|\n",
      "|Random_Acts_Of_Am...| 2357.632|  8142|\n",
      "|            buildapc|2257.9685|  8402|\n",
      "|               DotA2|2079.6892| 18848|\n",
      "|              movies|1959.7386| 19706|\n",
      "|electronic_cigarette|1865.1583|  7779|\n",
      "|       SquaredCircle|1733.7367| 17271|\n",
      "|       pokemontrades|1631.5997|  7578|\n",
      "|              hockey|1625.2948| 17844|\n",
      "+--------------------+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "analyzeDF.sort(func.col('analyze').desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare 2 Subreddits based on Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basketball Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # be sure to have stopwords installed for this using nltk.download_shell()\n",
    "import pandas as pd \n",
    "import string\n",
    "\n",
    "# This Cell takes 4 minutes...\n",
    "nltk.downloader.download('vader_lexicon')\n",
    "\n",
    "newsamp = df.filter(df.subreddit.like(\"nba\")&~df['author'].isin(['[deleted]']))\n",
    "iteratebody = newsamp.select(\"body\").rdd.flatMap(list).collect()\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# install Vader and make sure you download the lexicon as well\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "# this step will return an error if you have not installed the lexicon\n",
    "summary = {\"positive\":0,\"neutral\":0,\"negative\":0}\n",
    "for i, message in enumerate(iteratebody):    \n",
    "    \n",
    "    #clean the comments\n",
    "    message = pre_process(message)\n",
    "        \n",
    "    ss = sid.polarity_scores(message)\n",
    "    #for k in sorted (ss):\n",
    "    #    print(\"{0}:{1},\".format(k,ss[k]),end=\"/n \")\n",
    "    \n",
    "    if ss[\"compound\"] == 0.0: \n",
    "        summary[\"neutral\"] +=1\n",
    "    elif ss[\"compound\"] > 0.0:\n",
    "        summary[\"positive\"] +=1\n",
    "    else:\n",
    "        summary[\"negative\"] +=1\n",
    "        \n",
    "    i=i+1\n",
    "    #print(i); \n",
    "    \n",
    "        \n",
    "print(i);         \n",
    "print(summary);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
