{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Question-4 Toxicity (Sentiment Analysis)\n",
    "\n",
    "Toxicity: using Sentiment Analysis, determine the top 5 positive subreddits and top 5 negative subreddits based on comment sentiment.\n",
    "\n",
    "Choose two subreddits focused on similar topics but with different views, e.g., /r/apple and /r/android. Compare the toxicity of the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://i.imgur.com/MijPBwS.jpg \"Logo Title Text 1\")\n",
    "\n",
    "Sentiment analysis is the automatic process of identifying positive, negative and neutral emotions in text. Equipped with AI, sentiment analysis allows businesses to understand how customers feel about their products and services and extract valuable insights that lead to better decision-making.\n",
    "\n",
    "In a world where we generate 2.5 quintillion bytes of data every day, sentiment analysis has become a key tool for making sense of that data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results \n",
    "Lets share with you the results first then we can talk about how we implement it. \n",
    "\n",
    "![alt text](https://i.imgur.com/tsNhN6Z.jpg \"Logo Title Text 1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the graph above, most positive subreddit is \"AskReddit\" with 16001 sentiment score. AskReddit is the place to ask and answer thought-provoking questions. Actually, we were expecting that AskReddit will be the one of top but not expecting this much diffence between second top. Second top is \"leagueoflegends\" with 5536 sentiment score. And gaming, pics, gonewild, IAmA are the following top positive subreddits with 3954,3704,3236,3081 respectively. \n",
    "<br> \n",
    "On the other hand, top negative subreddit as expected \"news\" subreddit. \"news\" subreddit is real news articles, primarily but not exclusively, news relating to the United States and the rest of the World. So world is going crazy and news also. So news has -1589 negative sentiment score. The second top negative subreddit is also related with news \"worldnews\" with -1358. Third top negative subreddit is de which is related with Germany. We really didn't understand what is going on so bad in Germany. :) Other top negative subreddits are podemos, WTF, argentina, MensRights with -329, -201, -200, -198 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sampled original data to 1% for first part of this question. 1% data is enough to determine the top 5 positive subreddits and top 5 negative subreddits based on comment sentiment. Important Note: If there are less than 100 comments of a specific subreddit we didn't consider it in this analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.08 ms, sys: 1.22 ms, total: 3.3 ms\n",
      "Wall time: 7.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "originDF = sqlContext.read.json(\"hdfs://orion11:11001/sampled_reddit_v3/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the comment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def pre_process(text):\n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|[^\\\\w|\\\\s]|(\\_))+\",\"\",text)\n",
    "    text=re.sub(\"(\\\\s)+\",\" \",text)\n",
    "    #print(text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used NLTK /VADER library for Sentiment Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important Note: If there are less than 100 comments of a specific subreddit we didn't consider it in this analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home4/saozdamar/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk # be sure to have stopwords installed for this using nltk.download_shell()\n",
    "import string\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql import types as types\n",
    "\n",
    "# This Cell takes 4 minutes... (199923 records for soccer)\n",
    "nltk.downloader.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# install Vader and make sure you download the lexicon as well\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def calculatue_score(listBody):    \n",
    "    #0.1 sample\n",
    "    #newsamp = sqlDF.filter(sqlDF.subreddit.like(category)&~sqlDF['author'].isin(['[deleted]']))\n",
    "    \n",
    "    #0.01 sample\n",
    "    #newsamp = originDF.filter(originDF.subreddit.like(category)&~originDF['author'].isin(['[deleted]']))\n",
    "#     newsamp = originDF.filter(originDF['subreddit'].isin(category)&~originDF['author'].isin(['[deleted]']))    \n",
    "#     iteratebody = newsamp.select(\"body\").rdd.flatMap(list).collect()    \n",
    "    \n",
    "    if(len(listBody)>100):                            \n",
    "        # this step will return an error if you have not installed the lexicon \n",
    "        result = 0.0;\n",
    "        for message in listBody:   \n",
    "            #clean the comments\n",
    "            message = pre_process(message)\n",
    "\n",
    "            ss = sid.polarity_scores(message)\n",
    "            result += ss[\"compound\"]\n",
    "\n",
    "        #print(summary)\n",
    "        return result\n",
    "    else:\n",
    "        #print('Sorry less than 100 comments.')\n",
    "        return 0\n",
    "    \n",
    "calculatue_score_udf = func.udf(calculatue_score, types.FloatType())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing sentiment analysis for each subreddit category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-----+\n",
      "|          subreddit|  analyze|count|\n",
      "+-------------------+---------+-----+\n",
      "|            Amateur|  24.4157|  153|\n",
      "|              HPMOR|  22.6543|  162|\n",
      "|         MLBTheShow|  63.3394|  380|\n",
      "|         MensRights|-197.9045| 3299|\n",
      "|         NHLStreams|  28.2473|  171|\n",
      "|         QuotesPorn|  32.2833|  289|\n",
      "|       SaltLakeCity|  41.0967|  280|\n",
      "|UnresolvedMysteries| -21.0719|  461|\n",
      "|         WahoosTipi|  27.2657|  257|\n",
      "|              anime| 1483.797| 9240|\n",
      "|           lacrosse|  27.2878|  110|\n",
      "|       marvelheroes|  67.5551|  320|\n",
      "|         mistyfront|   0.3322|  131|\n",
      "|             travel| 451.8471| 1454|\n",
      "|            ukraina|  16.0944|  655|\n",
      "| Anarcho_Capitalism| 120.7049| 1526|\n",
      "|          BABYMETAL| 108.9668|  390|\n",
      "|      ClickerHeroes|  90.6882|  357|\n",
      "|             Hawaii|  45.1344|  237|\n",
      "|          JUSTNOMIL|  70.3518|  735|\n",
      "+-------------------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 33.8 ms, sys: 6.69 ms, total: 40.5 ms\n",
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql import functions as func\n",
    "\n",
    "analyzeDF = (originDF.groupBy('subreddit')\n",
    ".agg(calculatue_score_udf(func.collect_list('body')).alias('analyze'), func.count(func.lit(1)).alias('count')))\n",
    "analyzeDF = analyzeDF.filter(analyzeDF['count']>=100)\n",
    "analyzeDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(subreddit,StringType,true),StructField(analyze,FloatType,true),StructField(count,LongType,false)))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzeDF.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 5 Positive Subreddits\n",
    "\n",
    "As you see in the below. Top 5 positive subreddits are \"AskReddit\", \"leagueoflegends\", \"gaming\", \"pics\" and surpsiringly \"gonewild\". :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+------+\n",
      "|           subreddit|  analyze| count|\n",
      "+--------------------+---------+------+\n",
      "|           AskReddit|16001.021|284960|\n",
      "|     leagueoflegends| 5536.479| 48008|\n",
      "|              gaming|3953.6223| 41689|\n",
      "|                pics|3703.8025| 60123|\n",
      "|            gonewild|3235.5254| 13058|\n",
      "|                IAmA| 3080.609| 21385|\n",
      "|               funny|2940.4287| 64062|\n",
      "|                 nfl|2826.1062| 32719|\n",
      "|        pcmasterrace|2776.1965| 18199|\n",
      "|                 nba| 2721.648| 26823|\n",
      "|              soccer| 2531.825| 22251|\n",
      "|               trees|2440.6042| 17487|\n",
      "|Random_Acts_Of_Am...| 2357.632|  8142|\n",
      "|            buildapc|2257.9685|  8402|\n",
      "|               DotA2|2079.6892| 18848|\n",
      "|              movies|1959.7386| 19706|\n",
      "|electronic_cigarette|1865.1583|  7779|\n",
      "|       SquaredCircle|1733.7367| 17271|\n",
      "|       pokemontrades|1631.5997|  7578|\n",
      "|              hockey|1625.2948| 17844|\n",
      "+--------------------+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "analyzeDF.sort(func.col('analyze').desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 5 Negative Subreddits\n",
    "As you see in the below. Top 5 negative subreddits are news, worldnews, de, podemos, WTF. And top 6th negative is Argentiona?! Oh my god Why? They have Maradona and Messi. They should be happy :P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+-----+\n",
      "|       subreddit|   analyze|count|\n",
      "+----------------+----------+-----+\n",
      "|          Horses|      null|  100|\n",
      "|      letsgofish|      null|  100|\n",
      "|       AskNetsec|      null|  100|\n",
      "|  TACSdiscussion|      null|  100|\n",
      "|      manchester|      null|  100|\n",
      "| prisonarchitect|      null|  100|\n",
      "|            snes|      null|  100|\n",
      "|             rct|      null|  100|\n",
      "|     NewMarvelRp|      null|  100|\n",
      "|           tulsa|      null|  100|\n",
      "|            news|-1589.3267|24302|\n",
      "|       worldnews|-1358.1285|34472|\n",
      "|              de| -493.5001| 1790|\n",
      "|         podemos| -328.7741| 1635|\n",
      "|             WTF| -201.3484|35570|\n",
      "|       argentina| -199.9925| 1710|\n",
      "|      MensRights| -197.9045| 3299|\n",
      "|      conspiracy| -182.2452| 5431|\n",
      "|     JusticePorn| -127.9909| 1669|\n",
      "|Bad_Cop_No_Donut| -125.2861|  719|\n",
      "+----------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "analyzeDF.sort(func.col('analyze').asc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare 4 Subreddits based on Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to compare 4 different cities for Sentiment Analysis. We choosed San Francisco, Los Angeles, San Diego and San Jose. Before comparing the toxicity of these 4 cities we were expecting that San Diego will be most positive city among those 4. So the results didn't surprise us. Most positive city is San Diego and Most negative city is San Francisco. Second positive city is San Jose. So now, instead of staying in SF, we are thinking to move to San Jose :) \n",
    "\n",
    "You can see the whole results below pie chart:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://i.imgur.com/FgNWjFh.jpg \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home4/saozdamar/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk # be sure to have stopwords installed for this using nltk.download_shell()\n",
    "import pandas as pd \n",
    "import string\n",
    "\n",
    "nltk.downloader.download('vader_lexicon')\n",
    "def calculatue_score2(category):\n",
    "    newsamp = originDF.filter(originDF.subreddit.like(category)&~originDF['author'].isin(['[deleted]']))\n",
    "    iteratebody = newsamp.select(\"body\").rdd.flatMap(list).collect()\n",
    "\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    # install Vader and make sure you download the lexicon as well\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    # this step will return an error if you have not installed the lexicon\n",
    "    summary = {\"positive\":0,\"neutral\":0,\"negative\":0}\n",
    "    summary['category'] = category\n",
    "    i=0\n",
    "    for message in tqdm.tqdm((iteratebody)):   \n",
    "        i=i+1\n",
    "        #clean the comments\n",
    "        message = pre_process(message)\n",
    "        ss = sid.polarity_scores(message)\n",
    "\n",
    "        if ss[\"compound\"] == 0.0: \n",
    "            summary[\"neutral\"] +=1\n",
    "        elif ss[\"compound\"] > 0.0:\n",
    "            summary[\"positive\"] +=1\n",
    "        else:\n",
    "            summary[\"negative\"] +=1\n",
    "    \n",
    "    print('Total comments for ', category , ' is ' , i)\n",
    "    print(summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPUTING FOR :  sanfrancisco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 611/611 [00:00<00:00, 2823.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments for  sanfrancisco  is  611\n",
      "{'positive': 282, 'neutral': 120, 'negative': 209, 'category': 'sanfrancisco'}\n",
      "COMPUTING FOR :  LosAngeles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1148/1148 [00:00<00:00, 3286.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments for  LosAngeles  is  1148\n",
      "{'positive': 522, 'neutral': 297, 'negative': 329, 'category': 'LosAngeles'}\n",
      "COMPUTING FOR :  SanJose\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:00<00:00, 3466.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments for  SanJose  is  115\n",
      "{'positive': 56, 'neutral': 31, 'negative': 28, 'category': 'SanJose'}\n",
      "COMPUTING FOR :  sandiego\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 464/464 [00:00<00:00, 3212.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments for  sandiego  is  464\n",
      "{'positive': 243, 'neutral': 123, 'negative': 98, 'category': 'sandiego'}\n",
      "CPU times: user 836 ms, sys: 13.3 ms, total: 849 ms\n",
      "Wall time: 18.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'positive': 282,\n",
       "  'neutral': 120,\n",
       "  'negative': 209,\n",
       "  'category': 'sanfrancisco'},\n",
       " {'positive': 522, 'neutral': 297, 'negative': 329, 'category': 'LosAngeles'},\n",
       " {'positive': 56, 'neutral': 31, 'negative': 28, 'category': 'SanJose'},\n",
       " {'positive': 243, 'neutral': 123, 'negative': 98, 'category': 'sandiego'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import tqdm\n",
    "desired_categories = ['sanfrancisco','LosAngeles','SanJose','sandiego']\n",
    "scores = []\n",
    "for category in desired_categories:\n",
    "    print('COMPUTING FOR : ', category)\n",
    "    scores.append(calculatue_score2(category))\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
