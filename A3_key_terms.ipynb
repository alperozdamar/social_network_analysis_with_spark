{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Terms: \n",
    "Calculate the TF-IDF(Term Frequency-Inverse Document Frequency) for a given subreddit.\n",
    "\n",
    "Produce a Tag Cloud of the terms (note: this doesnâ€™t have to be integrated into your code; simply including the image is enough)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "import pyspark.sql.functions as func\n",
    "import re\n",
    "import math\n",
    "\n",
    "def pre_process(text):\n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|[^\\\\w|\\\\s]|(\\_))+\",\"\",text)\n",
    "    text=re.sub(\"(\\\\s)+\",\" \",text)\n",
    "    print(text)\n",
    "    return text.strip()\n",
    "\n",
    "def calc_idf(docCount, df):\n",
    "    return math.log((float(docCount) + 1) / (float(df) + 1))\n",
    "#     return math.log((float(docCount) + 1))\n",
    "\n",
    "pre_process_udf = func.udf(pre_process, StringType())\n",
    "calc_idf_udf = func.udf(calc_idf, DoubleType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data from sampled_reddit dataset (10% of whole dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.json(\"hdfs://orion11:11001/sampled_reddit/*\")\n",
    "# df = sqlContext.read.json(\"hdfs://orion11:11001/reddit/2006/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the list of subreddit with the number of comment in their. Then sort and get the subreddit that have the most comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n",
      "|      subreddit|Num Of Comments|\n",
      "+---------------+---------------+\n",
      "|      AskReddit|       28466878|\n",
      "|          funny|        6385225|\n",
      "|           pics|        6015627|\n",
      "|       politics|        5154982|\n",
      "|leagueoflegends|        4811568|\n",
      "|         gaming|        4165456|\n",
      "|            WTF|        3583591|\n",
      "|      worldnews|        3456812|\n",
      "|  AdviceAnimals|        3408410|\n",
      "|         videos|        3335506|\n",
      "|            nfl|        3251565|\n",
      "|  todayilearned|        2922639|\n",
      "|            nba|        2679799|\n",
      "|           news|        2411619|\n",
      "|         soccer|        2222209|\n",
      "|           IAmA|        2145773|\n",
      "|         movies|        1978592|\n",
      "|          DotA2|        1880917|\n",
      "|        atheism|        1878108|\n",
      "|   pcmasterrace|        1798732|\n",
      "+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subredditComments = (df\n",
    " .groupBy(df.subreddit)\n",
    " .agg(\n",
    "     func.count(func.lit(1)).alias(\"Num Of Comments\")\n",
    "     ))\n",
    "(subredditComments\n",
    " .sort(func.desc(\"Num Of Comments\"))\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base on the result, we will select AskReddit as the one to calculate TF-IDF\n",
    "Then we only select the body(comment) and generate id for each comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                body|doc_id|\n",
      "+--------------------+------+\n",
      "|i read the title ...|     0|\n",
      "|because youre abo...|     1|\n",
      "|flushing with you...|     2|\n",
      "|              me too|     3|\n",
      "|i dont know but m...|     4|\n",
      "|we make a differe...|     5|\n",
      "|isnt summer heigh...|     6|\n",
      "|i got mine out a ...|     7|\n",
      "|or show us you ex...|     8|\n",
      "|defiantly im not ...|     9|\n",
      "|pretty sure once ...|    10|\n",
      "|while this does s...|    11|\n",
      "|as long as they c...|    12|\n",
      "|i use formatfacto...|    13|\n",
      "|heman sings it be...|    14|\n",
      "|                 see|    15|\n",
      "|dont be shy cat s...|    16|\n",
      "|for the time it w...|    17|\n",
      "|i know i will nev...|    18|\n",
      "|yeah right story ...|    19|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26264703"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "askRedditComments = (df\n",
    "                     .filter(df.subreddit.like(\"AskReddit\") & ~df.body.like('[removed]') & ~df.body.like('[deleted]'))\n",
    "                     .select(df.body))\n",
    "# askRedditComments = df.filter(df.subreddit.like(\"reddit.com\")).select(df.body)\n",
    "# askRedditComments = (df\n",
    "#                      .filter(df.subreddit.like(\"slate\") & ~df.body.like('[removed]') & ~df.body.like('[deleted]'))\n",
    "#                      .select(df.body))\n",
    "\n",
    "askRedditComments = (askRedditComments\n",
    "                     .withColumn(\"body\", pre_process_udf(askRedditComments.body))\n",
    "                     .withColumn(\"doc_id\", func.monotonically_increasing_id()))\n",
    "docCount = askRedditComments.count()\n",
    "askRedditComments.show()\n",
    "askRedditComments.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------+\n",
      "|                body|doc_id|    word|\n",
      "+--------------------+------+--------+\n",
      "|i read the title ...|     0|       i|\n",
      "|i read the title ...|     0|    read|\n",
      "|i read the title ...|     0|     the|\n",
      "|i read the title ...|     0|   title|\n",
      "|i read the title ...|     0|     and|\n",
      "|i read the title ...|     0| thought|\n",
      "|i read the title ...|     0|      of|\n",
      "|i read the title ...|     0|    that|\n",
      "|i read the title ...|     0|cheating|\n",
      "|i read the title ...|     0|   bitch|\n",
      "|i read the title ...|     0|   clown|\n",
      "|i read the title ...|     0|    from|\n",
      "|i read the title ...|     0|     the|\n",
      "|i read the title ...|     0|glassjaw|\n",
      "|i read the title ...|     0|   video|\n",
      "|because youre abo...|     1| because|\n",
      "|because youre abo...|     1|   youre|\n",
      "|because youre abo...|     1|   about|\n",
      "|because youre abo...|     1|      to|\n",
      "|because youre abo...|     1|    wash|\n",
      "+--------------------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "commentsTokensDF = (askRedditComments\n",
    "                    .select(\"body\", \"doc_id\",func.explode(func.split(\"body\", \"\\s+\")).alias(\"word\")))\n",
    "commentsTokensDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the TF(Term Frenquency) - the number of time that work appear in each comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+---+\n",
      "|doc_id|                word| tf|\n",
      "+------+--------------------+---+\n",
      "|     4|              showed|  1|\n",
      "|     7|               thing|  2|\n",
      "|    21|                  do|  1|\n",
      "|    27|                 why|  1|\n",
      "|    42|                  as|  1|\n",
      "|    46|                 its|  1|\n",
      "|    50|                turn|  1|\n",
      "|    51|               start|  1|\n",
      "|    55|                 and|  2|\n",
      "|    91|                 gym|  1|\n",
      "|   129|thesehttptoiletcl...|  1|\n",
      "|   142|                time|  2|\n",
      "|   175|                fuck|  1|\n",
      "|   176|                  no|  1|\n",
      "|   183|               youre|  1|\n",
      "|   194|                kind|  1|\n",
      "|   207|                 the|  1|\n",
      "|   235|                sure|  1|\n",
      "|   240|                 age|  1|\n",
      "|   258|               under|  1|\n",
      "+------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordTf = (commentsTokensDF.groupBy(\"doc_id\",\"word\")\n",
    "        .agg(func.count(\"body\").alias(\"tf\")))\n",
    "wordTf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the DF(Document Frequency) - the number of comments that contain that word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|           word|   df|\n",
      "+---------------+-----+\n",
      "|          aaagh|   34|\n",
      "|         abbrev|   11|\n",
      "|   accumulation|  567|\n",
      "|        acidity|  515|\n",
      "|         aholes|  356|\n",
      "|      airheaded|   77|\n",
      "|      amplifier|  407|\n",
      "|          anime|17660|\n",
      "|       antennae|  254|\n",
      "|          apgar|   31|\n",
      "|      arguments|18758|\n",
      "|            art|47713|\n",
      "|          aruba|  218|\n",
      "|atheistsatheist|    1|\n",
      "|       autisitc|    3|\n",
      "|       backdate|   19|\n",
      "|        balding| 1749|\n",
      "|        baloons|   76|\n",
      "|        barrier| 5336|\n",
      "|      barristan|   91|\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordDf = (commentsTokensDF.groupBy(\"word\")\n",
    "        .agg(func.countDistinct(\"doc_id\").alias(\"df\")))\n",
    "wordDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base on DF, we will calculate IDF(Inverse Document Frequency)   \n",
    "IDF(t,D) = log[ (|D| + 1) / (DF(t,D) + 1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+------------------+\n",
      "|            word|   df|               idf|\n",
      "+----------------+-----+------------------+\n",
      "|    accumulation|  567|10.741615123834924|\n",
      "|         acidity|  515|10.837629777074513|\n",
      "|       advisoras|    1| 16.39058936199613|\n",
      "|          aholes|  356|11.206000760776437|\n",
      "|       amplifier|  407|11.072469368151914|\n",
      "|       anathesia|    4|15.474298630121975|\n",
      "|           anime|17660| 7.304622444859705|\n",
      "|           anodd|    5|15.291977073328022|\n",
      "|        antennae|  254| 11.54247299739765|\n",
      "|        apagaron|    1| 16.39058936199613|\n",
      "|      applejuice|   40|13.370164475851768|\n",
      "|       arguments|18758| 7.244307626320668|\n",
      "|     arrivedleft|    1| 16.39058936199613|\n",
      "|        arsholes|    3|15.697442181436186|\n",
      "|             art|47713|6.3107564076944325|\n",
      "|           aruba|  218|11.694664812739575|\n",
      "|asimovsilverberg|    2|15.985124253887966|\n",
      "|             avx|    8|14.886511965219857|\n",
      "|         ayyyyyy|  113| 12.34753809416158|\n",
      "|           baaaa|   62|12.940601816164543|\n",
      "+----------------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordIdf = (wordDf\n",
    "           .withColumn(\"idf\", calc_idf_udf(func.lit(docCount), wordDf.df)))\n",
    "wordIdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate TF-IDF base on wordTf and wordIdf above by multiply it  \n",
    "Which will generate a matrix of each word for document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordTfIdf = (wordTf\n",
    "      .join(wordIdf, [\"word\"])\n",
    "      .withColumn(\"tf_idf\", wordTf.tf * wordIdf.idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+---+------+---------------+----------------+\n",
      "|word|        doc_id| tf|    df|            idf|          tf_idf|\n",
      "+----+--------------+---+------+---------------+----------------+\n",
      "| two| 2018634641920| 21|527696|3.9074590082268|82.0566391727628|\n",
      "| two| 6012954220978| 21|527696|3.9074590082268|82.0566391727628|\n",
      "| two| 3496103386112| 16|527696|3.9074590082268|62.5193441316288|\n",
      "| two| 8521215117327| 16|527696|3.9074590082268|62.5193441316288|\n",
      "| two| 6356551598664| 15|527696|3.9074590082268| 58.611885123402|\n",
      "| two|16733192592064| 13|527696|3.9074590082268|50.7969671069484|\n",
      "| two|  884763267346| 12|527696|3.9074590082268|46.8895080987216|\n",
      "| two| 2516850844485| 12|527696|3.9074590082268|46.8895080987216|\n",
      "| two|16003048152114| 12|527696|3.9074590082268|46.8895080987216|\n",
      "| two| 1133871386678| 12|527696|3.9074590082268|46.8895080987216|\n",
      "| two| 2851858285444| 12|527696|3.9074590082268|46.8895080987216|\n",
      "| two|13838384632652| 12|527696|3.9074590082268|46.8895080987216|\n",
      "| two|11579231836661| 12|527696|3.9074590082268|46.8895080987216|\n",
      "| two|16655883176969| 11|527696|3.9074590082268|42.9820490904948|\n",
      "| two| 5669356846002| 11|527696|3.9074590082268|42.9820490904948|\n",
      "| two| 4509715669383| 11|527696|3.9074590082268|42.9820490904948|\n",
      "| two|13125420059804| 11|527696|3.9074590082268|42.9820490904948|\n",
      "| two|  455266535560| 11|527696|3.9074590082268|42.9820490904948|\n",
      "| two|11493332485245| 11|527696|3.9074590082268|42.9820490904948|\n",
      "| two| 5214090307597| 11|527696|3.9074590082268|42.9820490904948|\n",
      "+----+--------------+---+------+---------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordTfIdf.filter(wordTfIdf.word.like(\"two\")).sort(func.desc(\"tf_idf\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag clouds\n",
    "Which terms should i put into tag cloud and how the value will be determined"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
